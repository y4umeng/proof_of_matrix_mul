{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Az46fWbShJzM"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import hashlib\n",
        "import random\n",
        "import json\n",
        "import datetime\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize(x: t.Tensor, num_bits: int = 8) -> t.Tensor:\n",
        "  \"\"\"\n",
        "  Quantization using stochastic rounding.\n",
        "    - Divides the tensor into 2 ** num_bits - 1 bins (2 ** num_bits possible vals)\n",
        "      and randomly rounds to each val with probability proportional to distance from val\n",
        "    - Maintains unbiasedness\n",
        "  \"\"\"\n",
        "  x_min = x.min()\n",
        "  x_max = x.max()\n",
        "\n",
        "  if x_max == x_min:    # degenerate case, not likely unless size of tensor is 1\n",
        "    return x.clone()\n",
        "\n",
        "  bins = 2 ** num_bits - 1\n",
        "  scale = (x_max - x_min) / bins\n",
        "\n",
        "  x_scaled = (x - x_min) / scale\n",
        "  x_floor = t.floor(x_scaled)\n",
        "  x_rem = x_scaled - x_floor\n",
        "\n",
        "  rnd = t.rand_like(x_rem)\n",
        "  x_quantized = (x_floor + (rnd < x_rem).float()) * scale + x_min\n",
        "\n",
        "  return x_quantized"
      ],
      "metadata": {
        "id": "-cP2SfJcirPi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_test = t.rand(10) * 2 - 1     # from -1 to 1\n",
        "print(t.sort(t_test).values)\n",
        "\n",
        "t_quantized = quantize(t_test, num_bits=4)\n",
        "print(t.sort(t_quantized).values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRdBNmVGjjFe",
        "outputId": "e46c04bd-ea9b-4ce5-bb6d-5a6291197f59"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.8523, -0.7502, -0.7030, -0.3144, -0.2169, -0.2010,  0.0906,  0.5037,\n",
            "         0.5889,  0.6592])\n",
            "tensor([-0.8523, -0.7515, -0.6508, -0.3485, -0.2477, -0.1469,  0.0546,  0.4577,\n",
            "         0.6592,  0.6592])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hash_tensor(x: t.Tensor) -> str:\n",
        "  \"\"\"\n",
        "  Hash (SHA256) for a tensor.\n",
        "    - Current implementation has a lot of overhead\n",
        "    - First moving tensor to CPU, then converting to numpy, then hashing\n",
        "    - Ideas of batching tensors to hash?\n",
        "  \"\"\"\n",
        "  h = hashlib.sha256()\n",
        "  h.update(x.detach().cpu().numpy().tobytes())\n",
        "  return h.hexdigest()"
      ],
      "metadata": {
        "id": "lDjR902TkyXC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(hash_tensor(t_quantized))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNhBKX2-nFB9",
        "outputId": "16c62810-d089-4816-e415-d5dd3110161e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fd40cbee873ec7d4a971476bd4abfd2f3e07bf14ea5d5bdd2e71dbc4ad151a80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ProofLinear(nn.Module):\n",
        "  \"\"\"\n",
        "  nn.Linear wrapper that logs proofs for the matrix multiplication.\n",
        "    - hashes input when forward is called, then hashes output\n",
        "    - also contained hashed weights for verification\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               in_features: int,\n",
        "               out_features: int,\n",
        "               bias: bool = True,\n",
        "               is_quantized: bool = True,\n",
        "               num_bits: int = 8):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "    if is_quantized:\n",
        "      self.weight_quantized = quantize(self.linear.weight.data, num_bits=8)\n",
        "    else:\n",
        "      self.weight_quantized = self.linear.weight.data\n",
        "\n",
        "    self.weight_hash = hash_tensor(self.weight_quantized)\n",
        "    self.proof_records = []\n",
        "    self.is_quantized = is_quantized\n",
        "    self.num_bits = num_bits\n",
        "\n",
        "  def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "    input_hash = hash_tensor(x.detach())\n",
        "\n",
        "    y = self.linear(x)\n",
        "\n",
        "    if self.is_quantized:\n",
        "      x_to_hash = quantize(x, num_bits=self.num_bits)\n",
        "      y_to_hash = quantize(y, num_bits=self.num_bits)\n",
        "    else:\n",
        "      x_to_hash, y_to_hash = x, y\n",
        "\n",
        "    input_hash = hash_tensor(x_to_hash.detach())\n",
        "    output_hash = hash_tensor(y_to_hash.detach())\n",
        "\n",
        "    dic = {\n",
        "      \"module\": \"ProofLinear\",\n",
        "      \"input\": x.detach().clone(),\n",
        "      \"output\": y.detach().clone(),\n",
        "      \"is_quantized\": self.is_quantized\n",
        "    }\n",
        "\n",
        "    if self.is_quantized:\n",
        "      dic[\"weight_quantized\"] = self.weight_quantized.clone()\n",
        "      dic[\"input_quantized\"] = x_to_hash.clone()\n",
        "      dic[\"output_quantized\"] = y_to_hash.clone()\n",
        "\n",
        "    dic[\"weight_hash\"] = self.weight_hash\n",
        "    dic[\"input_hash\"] = input_hash\n",
        "    dic[\"output_hash\"] = output_hash\n",
        "\n",
        "    self.proof_records.append(dic)\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "nPmV22g9nJIr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wrap_model_linear(model: nn.Module, is_quantized: bool = True):\n",
        "  \"\"\"\n",
        "  BFS way to replace all linear layers with ProofLinear layers\n",
        "  - (haven't tested on more complex models, but should work)\n",
        "  \"\"\"\n",
        "  queue = [model]\n",
        "\n",
        "  while queue:\n",
        "    parent = queue.pop(0)\n",
        "\n",
        "    for name, child in list(parent.named_children()):\n",
        "      if isinstance(child, t.nn.Linear):\n",
        "        proof_linear = ProofLinear(\n",
        "          child.in_features,\n",
        "          child.out_features,\n",
        "          bias=(child.bias is not None),\n",
        "          is_quantized=is_quantized\n",
        "        )\n",
        "\n",
        "        proof_linear.linear.weight.data.copy_(child.weight.data)  # copying weight data\n",
        "        if child.bias is not None:\n",
        "          proof_linear.linear.bias.data.copy_(child.bias.data)\n",
        "\n",
        "        setattr(parent, name, proof_linear)\n",
        "      else:\n",
        "          queue.append(child)"
      ],
      "metadata": {
        "id": "2PfKLEgPMVfv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_proof_records(model: nn.Module):\n",
        "  \"\"\"\n",
        "  Getting all proof records from the model (as each ProofLinear has the proof_records attribute)\n",
        "  \"\"\"\n",
        "  records = []\n",
        "  for m in model.modules():\n",
        "    if hasattr(m, \"proof_records\"):\n",
        "      records.extend(m.proof_records)\n",
        "\n",
        "  return records"
      ],
      "metadata": {
        "id": "qyelTVFlNnyh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TensorEncoder(json.JSONEncoder):\n",
        "  \"\"\"\n",
        "  Helper class to serialize tensors to JSON\n",
        "  \"\"\"\n",
        "  def default(self, obj):\n",
        "    if isinstance(obj, t.Tensor):\n",
        "      return obj.cpu().tolist()\n",
        "\n",
        "    return super().default(obj)"
      ],
      "metadata": {
        "id": "AwTMpvFgRFZ5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_proof(model: nn.Module, sample_count_pct: int = 0.2, print_records: bool = False) -> bool:\n",
        "  \"\"\"\n",
        "  Gathers all proof records from the model, randomly samples some percent, recomputes hashes\n",
        "  and verifies them against the stored values\n",
        "  \"\"\"\n",
        "  proof_records = get_all_proof_records(model)\n",
        "  if not proof_records:\n",
        "    print(\"No proof records.\")\n",
        "    return False\n",
        "\n",
        "  sample_indices = random.sample(\n",
        "    range(len(proof_records)),\n",
        "    int(sample_count_pct * len(proof_records))\n",
        "  )\n",
        "  all_verified = True\n",
        "\n",
        "  for idx in sample_indices:\n",
        "    record = proof_records[idx]\n",
        "\n",
        "    # recomputing hashes\n",
        "    att = \"_quantized\" if record[\"is_quantized\"] else \"\"\n",
        "    recomputed_input_hash = hash_tensor(record[f\"input{att}\"])\n",
        "    recomputed_weight_hash = hash_tensor(record[f\"weight{att}\"])\n",
        "    recomputed_output_hash = hash_tensor(record[f\"output{att}\"])\n",
        "\n",
        "    proof_records[idx][\"recomputed_weight_hash\"] = recomputed_weight_hash\n",
        "    proof_records[idx][\"recomputed_input_hash\"] = recomputed_input_hash\n",
        "    proof_records[idx][\"recomputed_output_hash\"] = recomputed_output_hash\n",
        "\n",
        "    proof_records[idx][\"verified_weight_hash\"] = record[\"weight_hash\"] == recomputed_weight_hash\n",
        "    proof_records[idx][\"verified_input_hash\"] = record[\"input_hash\"] == recomputed_input_hash\n",
        "    proof_records[idx][\"verified_output_hash\"] = record[\"output_hash\"] == recomputed_output_hash\n",
        "\n",
        "    if not (proof_records[idx][\"verified_weight_hash\"] and\n",
        "      proof_records[idx][\"verified_input_hash\"]        and\n",
        "      proof_records[idx][\"verified_output_hash\"]):\n",
        "\n",
        "      all_verified = False\n",
        "\n",
        "  if print_records: # dumps proof records to json file\n",
        "    name = \"NA\"\n",
        "    if hasattr(model, \"name\"):\n",
        "      name = model.name\n",
        "\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "    with open(f\"proof_records_{name}_{timestamp}.json\", \"w\") as f:\n",
        "      json.dump(proof_records, f, cls=TensorEncoder, indent=4)\n",
        "\n",
        "  return all_verified"
      ],
      "metadata": {
        "id": "JZwSubTXOGld"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyModel(nn.Module):\n",
        "  \"\"\"\n",
        "  Mock dummy model for testing\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(128, 64)\n",
        "    self.linear2 = nn.Linear(64, 32)\n",
        "    self.linear3 = nn.Linear(32, 16)\n",
        "    self.linear4 = nn.Linear(16, 8)\n",
        "\n",
        "  def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "    x = self.linear1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.linear2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.linear3(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.linear4(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "3ecFHjp3PdCI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = DummyModel()\n",
        "test_model.name = \"test\"\n",
        "\n",
        "wrap_model_linear(test_model)\n",
        "\n",
        "test_input = t.randn(1, 128)\n",
        "test_output = test_model(test_input)\n",
        "\n",
        "verification_passed = verify_proof(test_model, sample_count_pct=1, print_records=True)\n",
        "\n",
        "print(verification_passed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I7I2UNLPkiO",
        "outputId": "d4ceb5de-b995-45e1-c61c-f91c47285ebb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gpu_time_ms(fn, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    run a CUDA kernel or tensor op and return elapsed time in milliseconds.\n",
        "    falls back to perf_counter on CPU tensors.\n",
        "    \"\"\"\n",
        "    if args and t.is_tensor(args[0]) and args[0].is_cuda:\n",
        "        # print(\"on gpu\")\n",
        "        start, end = t.cuda.Event(enable_timing=True), t.cuda.Event(enable_timing=True)\n",
        "        t.cuda.synchronize()\n",
        "        start.record()\n",
        "        out = fn(*args, **kwargs)\n",
        "        end.record()\n",
        "        t.cuda.synchronize()\n",
        "        return out, start.elapsed_time(end)\n",
        "    else:\n",
        "        print(\"on cpu\")\n",
        "        t0 = time.perf_counter()\n",
        "        out = fn(*args, **kwargs)\n",
        "        return out, (time.perf_counter() - t0)*1000\n",
        "\n",
        "class ProofLinearTimed(nn.Module):\n",
        "    \"\"\"\n",
        "    nn.Linear wrapper that:\n",
        "    • does the original proof logging (weights / inputs / outputs + hashes)\n",
        "    • records per‑call timing:\n",
        "        - matmul_ms : time spent in self.linear(x)\n",
        "        - hash_ms   : time spent in CPU SHA‑256 hashing\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, bias=True,\n",
        "                 is_quantized=True, num_bits=8):\n",
        "        super().__init__()\n",
        "        self.linear       = nn.Linear(in_features, out_features, bias=bias)\n",
        "        self.is_quantized = is_quantized\n",
        "        self.num_bits     = num_bits\n",
        "\n",
        "        self.weight_quantized = quantize(self.linear.weight.data, num_bits) \\\n",
        "                                if is_quantized else self.linear.weight.data\n",
        "        self.weight_hash      = hash_tensor(self.weight_quantized)\n",
        "        self.proof_records    = []\n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        #  measure the matrix multiplication\n",
        "        y, matmul_ms = gpu_time_ms(self.linear, x)\n",
        "\n",
        "        if self.is_quantized:\n",
        "            x_to_hash = quantize(x, self.num_bits)\n",
        "            y_to_hash = quantize(y, self.num_bits)\n",
        "        else:\n",
        "            x_to_hash, y_to_hash = x, y\n",
        "\n",
        "        #  measure hashing time\n",
        "        t0 = time.perf_counter()\n",
        "        input_hash  = hash_tensor(x_to_hash.detach())\n",
        "        output_hash = hash_tensor(y_to_hash.detach())\n",
        "        hash_ms = (time.perf_counter() - t0) * 1000\n",
        "\n",
        "        rec = dict(\n",
        "            module          = \"ProofLinearTimed\",\n",
        "            input           = x.detach().clone(),\n",
        "            output          = y.detach().clone(),\n",
        "            is_quantized    = self.is_quantized,\n",
        "            weight_hash     = self.weight_hash,\n",
        "            input_hash      = input_hash,\n",
        "            output_hash     = output_hash,\n",
        "            matmul_ms       = matmul_ms,\n",
        "            hash_ms         = hash_ms,\n",
        "        )\n",
        "        if self.is_quantized:\n",
        "            rec.update(\n",
        "                weight_quantized = self.weight_quantized.clone(),\n",
        "                input_quantized  = x_to_hash.clone(),\n",
        "                output_quantized = y_to_hash.clone()\n",
        "            )\n",
        "        self.proof_records.append(rec)\n",
        "        return y\n",
        "\n",
        "def wrap_model_linear_timed(model: nn.Module, is_quantized=True):\n",
        "    queue = [model]\n",
        "    while queue:\n",
        "        parent = queue.pop(0)\n",
        "        for name, child in list(parent.named_children()):\n",
        "            if isinstance(child, nn.Linear):\n",
        "                pl = ProofLinearTimed(child.in_features, child.out_features,\n",
        "                                      bias=(child.bias is not None),\n",
        "                                      is_quantized=is_quantized)\n",
        "                pl.linear.weight.data.copy_(child.weight.data)\n",
        "                if child.bias is not None:\n",
        "                    pl.linear.bias.data.copy_(child.bias.data)\n",
        "                setattr(parent, name, pl)\n",
        "            else:\n",
        "                queue.append(child)\n",
        "\n",
        "def summarize_timing(model: nn.Module):\n",
        "    records = get_all_proof_records(model)\n",
        "    if not records:\n",
        "        print(\"‑ no timing data recorded ‑\")\n",
        "        return\n",
        "    total_mm  = sum(r[\"matmul_ms\"] for r in records)\n",
        "    total_hash= sum(r[\"hash_ms\"]   for r in records)\n",
        "    print(f\"Total matmul time : {total_mm:9.3f} ms\")\n",
        "    print(f\"Total hash  time  : {total_hash:9.3f} ms\")\n",
        "    print(f\"Hashing overhead  : {100*total_hash/(total_mm+1e-6):6.2f} %\")\n",
        "    print(\"\\nPer‑layer breakdown (ms):\")\n",
        "    for i, r in enumerate(records):\n",
        "        print(f\"  layer {i:02d}  matmul={r['matmul_ms']:7.3f}   hash={r['hash_ms']:7.3f}\")\n",
        "\n",
        "model = DummyModel()\n",
        "wrap_model_linear_timed(model)\n",
        "model=model.cuda()\n",
        "\n",
        "x = t.randn(1, 128, device=\"cuda\")\n",
        "_ = model(x)\n",
        "\n",
        "summarize_timing(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcGoGELYQQDt",
        "outputId": "07f91951-bef0-4ffc-deb6-9c6b4be13190"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total matmul time :     0.473 ms\n",
            "Total hash  time  :     0.295 ms\n",
            "Hashing overhead  :  62.33 %\n",
            "\n",
            "Per‑layer breakdown (ms):\n",
            "  layer 00  matmul=  0.209   hash=  0.081\n",
            "  layer 01  matmul=  0.094   hash=  0.073\n",
            "  layer 02  matmul=  0.086   hash=  0.070\n",
            "  layer 03  matmul=  0.084   hash=  0.071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCHES       =  1        # mini‑batch size\n",
        "FEATURES_IN   = 4096      # width of first layer\n",
        "REPEATS       = 50        # number of forward passes\n",
        "\n",
        "cuda = t.cuda.is_available()\n",
        "device = \"cuda\" if cuda else \"cpu\"\n",
        "print(\"Running on\", device.upper())\n",
        "\n",
        "class BiggerDummy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(FEATURES_IN, 2048)\n",
        "        self.l2 = nn.Linear(2048, 1024)\n",
        "        self.l3 = nn.Linear(1024, 512)\n",
        "        self.l4 = nn.Linear(512, 256)\n",
        "    def forward(self,x):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = F.relu(self.l3(x))\n",
        "        return self.l4(x)\n",
        "\n",
        "model = BiggerDummy()\n",
        "wrap_model_linear_timed(model, is_quantized=True)\n",
        "model = model.to(device)\n",
        "\n",
        "# warm up\n",
        "x = t.randn(BATCHES, FEATURES_IN, device=device)\n",
        "_ = model(x)\n",
        "\n",
        "for m in model.modules():\n",
        "    if hasattr(m, \"proof_records\"):\n",
        "        m.proof_records.clear()\n",
        "\n",
        "for _ in range(REPEATS):\n",
        "    x = t.randn(BATCHES, FEATURES_IN, device=device)\n",
        "    _ = model(x)\n",
        "\n",
        "summarize_timing(model)\n",
        "\n",
        "def bench_one(shape, repeats=100):\n",
        "    a = t.randn(*shape, device=device)\n",
        "    b = t.randn(shape[-1], shape[-1]//2, device=device)\n",
        "\n",
        "    _, gemm_ms = gpu_time_ms(t.mm, a, b)\n",
        "\n",
        "    # measure hashing\n",
        "    t0 = time.perf_counter()\n",
        "    for _ in range(repeats):\n",
        "        _ = hash_tensor(a)\n",
        "    hash_ms = (time.perf_counter() - t0)*1000 / repeats\n",
        "\n",
        "    print(f\"Shape {shape}  GEMM {gemm_ms:6.3f} ms   hash {hash_ms:6.3f} ms   \"\n",
        "          f\"hash/GEMM ≈ {hash_ms/gemm_ms:4.1f}×\")\n",
        "\n",
        "for shape in [(1,512), (1,4096), (32,4096),(64,8192)]:\n",
        "    bench_one(shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mFFvwraQXmB",
        "outputId": "ee08345e-c2ba-4bfb-bb8b-2427816583a9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on CUDA\n",
            "Total matmul time :    24.220 ms\n",
            "Total hash  time  :    23.502 ms\n",
            "Hashing overhead  :  97.03 %\n",
            "\n",
            "Per‑layer breakdown (ms):\n",
            "  layer 00  matmul=  0.202   hash=  0.162\n",
            "  layer 01  matmul=  0.192   hash=  0.158\n",
            "  layer 02  matmul=  0.216   hash=  0.250\n",
            "  layer 03  matmul=  0.188   hash=  0.150\n",
            "  layer 04  matmul=  0.190   hash=  0.150\n",
            "  layer 05  matmul=  0.190   hash=  0.150\n",
            "  layer 06  matmul=  0.190   hash=  0.162\n",
            "  layer 07  matmul=  0.193   hash=  0.158\n",
            "  layer 08  matmul=  0.197   hash=  0.156\n",
            "  layer 09  matmul=  0.214   hash=  0.144\n",
            "  layer 10  matmul=  0.202   hash=  0.155\n",
            "  layer 11  matmul=  0.205   hash=  0.175\n",
            "  layer 12  matmul=  0.202   hash=  0.145\n",
            "  layer 13  matmul=  0.202   hash=  0.170\n",
            "  layer 14  matmul=  0.209   hash=  0.143\n",
            "  layer 15  matmul=  0.199   hash=  0.143\n",
            "  layer 16  matmul=  0.201   hash=  0.144\n",
            "  layer 17  matmul=  0.200   hash=  0.164\n",
            "  layer 18  matmul=  0.202   hash=  0.194\n",
            "  layer 19  matmul=  0.197   hash=  0.178\n",
            "  layer 20  matmul=  0.202   hash=  0.145\n",
            "  layer 21  matmul=  0.203   hash=  0.162\n",
            "  layer 22  matmul=  0.197   hash=  0.143\n",
            "  layer 23  matmul=  0.199   hash=  0.143\n",
            "  layer 24  matmul=  0.201   hash=  0.145\n",
            "  layer 25  matmul=  0.199   hash=  0.143\n",
            "  layer 26  matmul=  0.201   hash=  0.143\n",
            "  layer 27  matmul=  0.202   hash=  0.144\n",
            "  layer 28  matmul=  0.202   hash=  0.142\n",
            "  layer 29  matmul=  0.201   hash=  0.143\n",
            "  layer 30  matmul=  0.219   hash=  0.159\n",
            "  layer 31  matmul=  0.196   hash=  0.170\n",
            "  layer 32  matmul=  0.204   hash=  0.144\n",
            "  layer 33  matmul=  0.229   hash=  0.217\n",
            "  layer 34  matmul=  0.232   hash=  0.244\n",
            "  layer 35  matmul=  0.254   hash=  0.249\n",
            "  layer 36  matmul=  0.202   hash=  0.146\n",
            "  layer 37  matmul=  0.206   hash=  0.170\n",
            "  layer 38  matmul=  0.201   hash=  0.147\n",
            "  layer 39  matmul=  0.206   hash=  0.165\n",
            "  layer 40  matmul=  0.205   hash=  0.144\n",
            "  layer 41  matmul=  0.205   hash=  0.144\n",
            "  layer 42  matmul=  0.203   hash=  0.143\n",
            "  layer 43  matmul=  0.199   hash=  0.169\n",
            "  layer 44  matmul=  0.206   hash=  0.144\n",
            "  layer 45  matmul=  0.197   hash=  0.153\n",
            "  layer 46  matmul=  0.205   hash=  0.144\n",
            "  layer 47  matmul=  0.199   hash=  0.145\n",
            "  layer 48  matmul=  0.203   hash=  0.162\n",
            "  layer 49  matmul=  0.203   hash=  0.145\n",
            "  layer 50  matmul=  0.143   hash=  0.190\n",
            "  layer 51  matmul=  0.117   hash=  0.126\n",
            "  layer 52  matmul=  0.105   hash=  0.114\n",
            "  layer 53  matmul=  0.104   hash=  0.112\n",
            "  layer 54  matmul=  0.098   hash=  0.116\n",
            "  layer 55  matmul=  0.099   hash=  0.113\n",
            "  layer 56  matmul=  0.098   hash=  0.113\n",
            "  layer 57  matmul=  0.100   hash=  0.109\n",
            "  layer 58  matmul=  0.100   hash=  0.109\n",
            "  layer 59  matmul=  0.113   hash=  0.109\n",
            "  layer 60  matmul=  0.121   hash=  0.176\n",
            "  layer 61  matmul=  0.100   hash=  0.109\n",
            "  layer 62  matmul=  0.099   hash=  0.119\n",
            "  layer 63  matmul=  0.100   hash=  0.109\n",
            "  layer 64  matmul=  0.100   hash=  0.114\n",
            "  layer 65  matmul=  0.100   hash=  0.127\n",
            "  layer 66  matmul=  0.108   hash=  0.109\n",
            "  layer 67  matmul=  0.104   hash=  0.108\n",
            "  layer 68  matmul=  0.099   hash=  0.109\n",
            "  layer 69  matmul=  0.099   hash=  0.109\n",
            "  layer 70  matmul=  0.113   hash=  0.107\n",
            "  layer 71  matmul=  0.097   hash=  0.109\n",
            "  layer 72  matmul=  0.100   hash=  0.113\n",
            "  layer 73  matmul=  0.100   hash=  0.126\n",
            "  layer 74  matmul=  0.098   hash=  0.110\n",
            "  layer 75  matmul=  0.099   hash=  0.109\n",
            "  layer 76  matmul=  0.097   hash=  0.107\n",
            "  layer 77  matmul=  0.099   hash=  0.108\n",
            "  layer 78  matmul=  0.098   hash=  0.108\n",
            "  layer 79  matmul=  0.098   hash=  0.110\n",
            "  layer 80  matmul=  0.098   hash=  0.108\n",
            "  layer 81  matmul=  0.098   hash=  0.106\n",
            "  layer 82  matmul=  0.127   hash=  0.172\n",
            "  layer 83  matmul=  0.123   hash=  0.170\n",
            "  layer 84  matmul=  0.141   hash=  0.189\n",
            "  layer 85  matmul=  0.119   hash=  0.133\n",
            "  layer 86  matmul=  0.107   hash=  0.115\n",
            "  layer 87  matmul=  0.103   hash=  0.114\n",
            "  layer 88  matmul=  0.104   hash=  0.109\n",
            "  layer 89  matmul=  0.109   hash=  0.136\n",
            "  layer 90  matmul=  0.102   hash=  0.111\n",
            "  layer 91  matmul=  0.101   hash=  0.110\n",
            "  layer 92  matmul=  0.118   hash=  0.108\n",
            "  layer 93  matmul=  0.101   hash=  0.110\n",
            "  layer 94  matmul=  0.104   hash=  0.108\n",
            "  layer 95  matmul=  0.101   hash=  0.110\n",
            "  layer 96  matmul=  0.103   hash=  0.145\n",
            "  layer 97  matmul=  0.105   hash=  0.114\n",
            "  layer 98  matmul=  0.108   hash=  0.115\n",
            "  layer 99  matmul=  0.103   hash=  0.111\n",
            "  layer 100  matmul=  0.095   hash=  0.103\n",
            "  layer 101  matmul=  0.088   hash=  0.116\n",
            "  layer 102  matmul=  0.082   hash=  0.099\n",
            "  layer 103  matmul=  0.078   hash=  0.095\n",
            "  layer 104  matmul=  0.078   hash=  0.095\n",
            "  layer 105  matmul=  0.078   hash=  0.096\n",
            "  layer 106  matmul=  0.240   hash=  0.094\n",
            "  layer 107  matmul=  0.080   hash=  0.094\n",
            "  layer 108  matmul=  0.096   hash=  0.093\n",
            "  layer 109  matmul=  0.076   hash=  0.092\n",
            "  layer 110  matmul=  0.169   hash=  0.095\n",
            "  layer 111  matmul=  0.078   hash=  0.094\n",
            "  layer 112  matmul=  0.081   hash=  0.094\n",
            "  layer 113  matmul=  0.093   hash=  0.093\n",
            "  layer 114  matmul=  0.080   hash=  0.094\n",
            "  layer 115  matmul=  0.077   hash=  0.093\n",
            "  layer 116  matmul=  0.077   hash=  0.093\n",
            "  layer 117  matmul=  0.082   hash=  0.096\n",
            "  layer 118  matmul=  0.078   hash=  0.093\n",
            "  layer 119  matmul=  0.112   hash=  0.119\n",
            "  layer 120  matmul=  0.079   hash=  0.092\n",
            "  layer 121  matmul=  0.076   hash=  0.092\n",
            "  layer 122  matmul=  0.077   hash=  0.092\n",
            "  layer 123  matmul=  0.077   hash=  0.096\n",
            "  layer 124  matmul=  0.077   hash=  0.124\n",
            "  layer 125  matmul=  0.076   hash=  0.134\n",
            "  layer 126  matmul=  0.080   hash=  0.097\n",
            "  layer 127  matmul=  0.078   hash=  0.097\n",
            "  layer 128  matmul=  0.075   hash=  0.106\n",
            "  layer 129  matmul=  0.092   hash=  0.089\n",
            "  layer 130  matmul=  0.076   hash=  0.090\n",
            "  layer 131  matmul=  0.077   hash=  0.095\n",
            "  layer 132  matmul=  0.147   hash=  0.140\n",
            "  layer 133  matmul=  0.128   hash=  0.174\n",
            "  layer 134  matmul=  0.138   hash=  0.160\n",
            "  layer 135  matmul=  0.084   hash=  0.095\n",
            "  layer 136  matmul=  0.082   hash=  0.093\n",
            "  layer 137  matmul=  0.080   hash=  0.092\n",
            "  layer 138  matmul=  0.080   hash=  0.090\n",
            "  layer 139  matmul=  0.080   hash=  0.093\n",
            "  layer 140  matmul=  0.127   hash=  0.091\n",
            "  layer 141  matmul=  0.077   hash=  0.091\n",
            "  layer 142  matmul=  0.079   hash=  0.092\n",
            "  layer 143  matmul=  0.078   hash=  0.091\n",
            "  layer 144  matmul=  0.078   hash=  0.109\n",
            "  layer 145  matmul=  0.098   hash=  0.094\n",
            "  layer 146  matmul=  0.078   hash=  0.090\n",
            "  layer 147  matmul=  0.079   hash=  0.089\n",
            "  layer 148  matmul=  0.078   hash=  0.091\n",
            "  layer 149  matmul=  0.094   hash=  0.091\n",
            "  layer 150  matmul=  0.084   hash=  0.087\n",
            "  layer 151  matmul=  0.124   hash=  0.129\n",
            "  layer 152  matmul=  0.081   hash=  0.103\n",
            "  layer 153  matmul=  0.081   hash=  0.086\n",
            "  layer 154  matmul=  0.077   hash=  0.083\n",
            "  layer 155  matmul=  0.080   hash=  0.083\n",
            "  layer 156  matmul=  0.079   hash=  0.096\n",
            "  layer 157  matmul=  0.076   hash=  0.091\n",
            "  layer 158  matmul=  0.076   hash=  0.083\n",
            "  layer 159  matmul=  0.075   hash=  0.092\n",
            "  layer 160  matmul=  0.080   hash=  0.101\n",
            "  layer 161  matmul=  0.091   hash=  0.083\n",
            "  layer 162  matmul=  0.076   hash=  0.101\n",
            "  layer 163  matmul=  0.079   hash=  0.081\n",
            "  layer 164  matmul=  0.114   hash=  0.083\n",
            "  layer 165  matmul=  0.076   hash=  0.082\n",
            "  layer 166  matmul=  0.078   hash=  0.082\n",
            "  layer 167  matmul=  0.078   hash=  0.102\n",
            "  layer 168  matmul=  0.077   hash=  0.109\n",
            "  layer 169  matmul=  0.077   hash=  0.084\n",
            "  layer 170  matmul=  0.076   hash=  0.082\n",
            "  layer 171  matmul=  0.104   hash=  0.080\n",
            "  layer 172  matmul=  0.080   hash=  0.086\n",
            "  layer 173  matmul=  0.077   hash=  0.081\n",
            "  layer 174  matmul=  0.084   hash=  0.085\n",
            "  layer 175  matmul=  0.076   hash=  0.082\n",
            "  layer 176  matmul=  0.076   hash=  0.083\n",
            "  layer 177  matmul=  0.076   hash=  0.079\n",
            "  layer 178  matmul=  0.120   hash=  0.082\n",
            "  layer 179  matmul=  0.077   hash=  0.081\n",
            "  layer 180  matmul=  0.076   hash=  0.080\n",
            "  layer 181  matmul=  0.076   hash=  0.079\n",
            "  layer 182  matmul=  0.119   hash=  0.130\n",
            "  layer 183  matmul=  0.122   hash=  0.140\n",
            "  layer 184  matmul=  0.136   hash=  0.127\n",
            "  layer 185  matmul=  0.080   hash=  0.082\n",
            "  layer 186  matmul=  0.079   hash=  0.091\n",
            "  layer 187  matmul=  0.086   hash=  0.094\n",
            "  layer 188  matmul=  0.076   hash=  0.080\n",
            "  layer 189  matmul=  0.078   hash=  0.083\n",
            "  layer 190  matmul=  0.078   hash=  0.081\n",
            "  layer 191  matmul=  0.078   hash=  0.081\n",
            "  layer 192  matmul=  0.076   hash=  0.097\n",
            "  layer 193  matmul=  0.088   hash=  0.080\n",
            "  layer 194  matmul=  0.076   hash=  0.091\n",
            "  layer 195  matmul=  0.078   hash=  0.082\n",
            "  layer 196  matmul=  0.079   hash=  0.080\n",
            "  layer 197  matmul=  0.077   hash=  0.079\n",
            "  layer 198  matmul=  0.078   hash=  0.081\n",
            "  layer 199  matmul=  0.078   hash=  0.080\n",
            "Shape (1, 512)  GEMM  0.065 ms   hash  0.026 ms   hash/GEMM ≈  0.4×\n",
            "Shape (1, 4096)  GEMM  0.241 ms   hash  0.065 ms   hash/GEMM ≈  0.3×\n",
            "Shape (32, 4096)  GEMM  0.330 ms   hash  1.537 ms   hash/GEMM ≈  4.7×\n",
            "Shape (64, 8192)  GEMM  1.630 ms   hash  6.026 ms   hash/GEMM ≈  3.7×\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# benchmark GPU matmul vs. CPU‑hash for several batch sizes\n",
        "import torch, time, pandas as pd\n",
        "\n",
        "device      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "FEATURES_IN = 4096        # width of first layer\n",
        "REPEATS     = 30          # timed forward passes per batch size\n",
        "BATCH_LIST  = [1, 2, 4, 8, 16, 32, 64, 128]\n",
        "\n",
        "def run_once(batch_sz):\n",
        "    model = BiggerDummy().to(device)\n",
        "    wrap_model_linear_timed(model, is_quantized=True)\n",
        "\n",
        "    # 1‑‑ warm‑up (ignored in stats)\n",
        "    model = BiggerDummy()\n",
        "    wrap_model_linear_timed(model, is_quantized=True)\n",
        "    model = model.to(device)\n",
        "    _ = model(torch.randn(batch_sz, FEATURES_IN, device=device))\n",
        "\n",
        "    # clear records written by warm‑up\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"proof_records\"):\n",
        "            m.proof_records.clear()\n",
        "\n",
        "    # 2 timed loop\n",
        "    for _ in range(REPEATS):\n",
        "        _ = model(torch.randn(batch_sz, FEATURES_IN, device=device))\n",
        "\n",
        "    # 3 aggregate timings\n",
        "    recs = get_all_proof_records(model)\n",
        "    total_mm   = sum(r[\"matmul_ms\"] for r in recs) / REPEATS\n",
        "    total_hash = sum(r[\"hash_ms\"]   for r in recs) / REPEATS\n",
        "    overhead   = 100 * total_hash / (total_mm + 1e-6)\n",
        "    return total_mm, total_hash, overhead\n",
        "\n",
        "rows = []\n",
        "for B in BATCH_LIST:\n",
        "    mm, h, o = run_once(B)\n",
        "    rows.append(dict(batch=B, matmul_ms=mm, hash_ms=h, overhead_pct=o))\n",
        "    print(f\"batch={B:>3}   GEMM={mm:6.3f} ms   hash={h:6.3f} ms   overhead={o:5.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzjek_MARjGl",
        "outputId": "89874d2e-eba1-4d8a-f2cc-a25ac0f4b816"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch=  1   GEMM= 0.543 ms   hash= 0.537 ms   overhead= 98.8%\n",
            "batch=  2   GEMM= 0.511 ms   hash= 0.640 ms   overhead=125.4%\n",
            "batch=  4   GEMM= 0.610 ms   hash= 0.913 ms   overhead=149.7%\n",
            "batch=  8   GEMM= 0.714 ms   hash= 1.457 ms   overhead=204.1%\n",
            "batch= 16   GEMM= 0.800 ms   hash= 2.552 ms   overhead=318.9%\n",
            "batch= 32   GEMM= 0.860 ms   hash= 6.964 ms   overhead=809.9%\n",
            "batch= 64   GEMM= 1.196 ms   hash=14.517 ms   overhead=1213.4%\n",
            "batch=128   GEMM= 1.579 ms   hash=20.192 ms   overhead=1279.0%\n"
          ]
        }
      ]
    }
  ]
}