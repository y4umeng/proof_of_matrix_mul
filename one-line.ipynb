{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JoKa-ZhGYwtd"
      },
      "outputs": [],
      "source": [
        "# this would be implemented in a package called pomm and these functions would be available after importing\n",
        "# pomm\n",
        "import contextlib, torch, random, time, types, threading\n",
        "import torch.nn as nn\n",
        "from functools import wraps\n",
        "from torch.nn.modules.module import register_module_forward_hook\n",
        "\n",
        "_THREAD = threading.local()\n",
        "\n",
        "def sha256_cpu(x):\n",
        "    import hashlib\n",
        "    h = hashlib.sha256(); h.update(x.detach().cpu().numpy().tobytes())\n",
        "    return h.hexdigest()\n",
        "\n",
        "def _record(op, a, b, out, cfg):\n",
        "    if random.random() > cfg.sample_rate:\n",
        "        return\n",
        "    t0 = time.perf_counter()\n",
        "    h_in  = sha256_cpu(a)\n",
        "    h_out = sha256_cpu(out)\n",
        "    h_ms  = (time.perf_counter()-t0)*1000\n",
        "    _THREAD.records.append(dict(op=op,\n",
        "                                ain=a.shape, aout=out.shape,\n",
        "                                hin=h_in, hout=h_out,\n",
        "                                hash_ms=h_ms))\n",
        "\n",
        "def _wrap_fn(fn, op_name, cfg):\n",
        "    @wraps(fn)\n",
        "    def wrapper(*args, **kw):\n",
        "        out = fn(*args, **kw)\n",
        "        _record(op_name, args[0], args[1], out, cfg)\n",
        "        return out\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def verification(sample_rate=0.2):\n",
        "    cfg = types.SimpleNamespace(sample_rate=sample_rate)\n",
        "    _THREAD.records = []\n",
        "\n",
        "    # 1) monkey‑patch tensor matmuls\n",
        "    patched = []\n",
        "    for name in (\"mm\", \"matmul\", \"bmm\"):\n",
        "        orig = getattr(torch, name)\n",
        "        setattr(torch, name, _wrap_fn(orig, name, cfg))\n",
        "        patched.append((torch, name, orig))\n",
        "\n",
        "\n",
        "    def _linear_hook(module, inputs, output):\n",
        "      # this hook fires for EVERY module; only act on Linear\n",
        "      if isinstance(module, nn.Linear):\n",
        "          _record(\"linear\",\n",
        "                  inputs[0],            # in activations\n",
        "                  module.weight.t(),    # weight matrix\n",
        "                  output,               # out activations\n",
        "                  cfg)\n",
        "\n",
        "    hook_handle = register_module_forward_hook(_linear_hook)\n",
        "\n",
        "    try:\n",
        "        yield _THREAD.records     # user code runs here\n",
        "    finally:\n",
        "        for tgt, name, orig in patched:\n",
        "            setattr(tgt, name, orig)\n",
        "        hook_handle.remove()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiEU3vMZYy5a",
        "outputId": "73ee193a-710b-46a3-e06b-0de9e6988de9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "records: 2\n",
            "dict_keys(['op', 'ain', 'aout', 'hin', 'hout', 'hash_ms'])\n"
          ]
        }
      ],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "\n",
        "class Tiny(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(128, 64)\n",
        "        self.l2 = nn.Linear(64, 32)\n",
        "    def forward(self, x):\n",
        "        return self.l2(F.relu(self.l1(x)))\n",
        "\n",
        "model = Tiny().cuda()\n",
        "x = torch.randn(4, 128, device=\"cuda\")\n",
        "\n",
        "with verification(sample_rate=1.0) as recs:\n",
        "    y = model(x)\n",
        "\n",
        "print(\"records:\", len(recs))\n",
        "print(recs[0].keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yql8YVAZZjyh",
        "outputId": "67889165-a42e-42b8-968e-f0858bb9ffa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "baseline 0.267 ms | with‑hash 0.583 ms | overhead 118.8%\n"
          ]
        }
      ],
      "source": [
        "def timeit(fn, *a, iters=50, **kw):\n",
        "    import time, torch\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    for _ in range(iters):\n",
        "        fn(*a, **kw)\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter()-t0)*1000/iters\n",
        "\n",
        "# baseline\n",
        "base_ms = timeit(model, x, iters=100)\n",
        "\n",
        "# with POMM\n",
        "with verification(sample_rate=1.0):\n",
        "    pomm_ms = timeit(model, x, iters=100)\n",
        "\n",
        "print(f\"baseline {base_ms:.3f} ms | with-hash {pomm_ms:.3f} ms | overhead {(pomm_ms-base_ms)/base_ms*100:5.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HVbXITsCdmtX"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiXffRpwgUWh"
      },
      "source": [
        "Running this using GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxw-cFbxaSNT",
        "outputId": "4bd48e1c-c969-41dc-ac32-1e2f9e5b9e45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch, time, transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# from pomm import verification   what we would use if pomm was a package\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer  = AutoTokenizer.from_pretrained(model_name)\n",
        "model      = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "model.eval()\n",
        "\n",
        "BATCH   = 16\n",
        "SEQ_LEN = 32\n",
        "\n",
        "prompt_text = \"The quick brown fox jumps over the lazy dog. \" * 4\n",
        "tokens  = tokenizer(prompt_text, return_tensors=\"pt\")[\"input_ids\"][0][:SEQ_LEN]\n",
        "inputs  = tokens.unsqueeze(0).repeat(BATCH, 1).to(device)   # (B, L)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCChm3pcdrVN",
        "outputId": "38805fba-8900-4484-b993-030f528ec953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline  (no POMM):  41.12 ms / batch\n",
            "With POMM (sha256): 506.18 ms / batch\n",
            "Overhead: 1130.8%\n",
            "\n",
            "Hash records collected: 20\n",
            "First record example:\n",
            "{'op': 'linear', 'ain': torch.Size([16, 32, 768]), 'aout': torch.Size([16, 32, 50257]), 'hash_ms': 537.5580359998366}\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def timed_forward(model, inputs, iters=20):\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    for _ in range(iters):\n",
        "        _ = model(inputs).logits\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter() - t0) * 1000 / iters   # ms / batch\n",
        "\n",
        "# baseline\n",
        "base_ms = timed_forward(model, inputs, iters=20)\n",
        "print(f\"Baseline  (no POMM): {base_ms:6.2f} ms / batch\")\n",
        "\n",
        "# using the verification\n",
        "with verification(sample_rate=1.0) as records:\n",
        "    pomm_ms = timed_forward(model, inputs, iters=20)\n",
        "\n",
        "print(f\"With POMM (sha256): {pomm_ms:6.2f} ms / batch\")\n",
        "print(f\"Overhead: {(pomm_ms - base_ms) / base_ms * 100:5.1f}%\")\n",
        "\n",
        "print(f\"\\nHash records collected: {len(records)}\")\n",
        "print(\"First record example:\")\n",
        "print({k: v for k, v in records[0].items() if k not in ('hin','hout')})\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
