{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Az46fWbShJzM"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import hashlib\n",
        "import random\n",
        "import json\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize(x: t.Tensor, num_bits: int = 8) -> t.Tensor:\n",
        "  \"\"\"\n",
        "  Quantization using stochastic rounding.\n",
        "    - Divides the tensor into 2 ** num_bits - 1 bins (2 ** num_bits possible vals)\n",
        "      and randomly rounds to each val with probability proportional to distance from val\n",
        "    - Maintains unbiasedness\n",
        "  \"\"\"\n",
        "  x_min = x.min()\n",
        "  x_max = x.max()\n",
        "\n",
        "  if x_max == x_min:    # degenerate case, not likely unless size of tensor is 1\n",
        "    return x.clone()\n",
        "\n",
        "  bins = 2 ** num_bits - 1\n",
        "  scale = (x_max - x_min) / bins\n",
        "\n",
        "  x_scaled = (x - x_min) / scale\n",
        "  x_floor = t.floor(x_scaled)\n",
        "  x_rem = x_scaled - x_floor\n",
        "\n",
        "  rnd = t.rand_like(x_rem)\n",
        "  x_quantized = (x_floor + (rnd < x_rem).float()) * scale + x_min\n",
        "\n",
        "  return x_quantized"
      ],
      "metadata": {
        "id": "-cP2SfJcirPi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_test = t.rand(10) * 2 - 1     # from -1 to 1\n",
        "print(t.sort(t_test).values)\n",
        "\n",
        "t_quantized = quantize(t_test, num_bits=4)\n",
        "print(t.sort(t_quantized).values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRdBNmVGjjFe",
        "outputId": "e41dd16a-2d21-4a40-a1d0-aa88688f9719"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.7076, -0.6072,  0.0858,  0.1010,  0.2929,  0.3091,  0.4286,  0.6552,\n",
            "         0.9068,  0.9666])\n",
            "tensor([-0.7076, -0.5960,  0.0737,  0.0737,  0.2969,  0.2969,  0.5201,  0.6317,\n",
            "         0.9666,  0.9666])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hash_tensor(x: t.Tensor) -> str:\n",
        "  \"\"\"\n",
        "  Hash (SHA256) for a tensor.\n",
        "    - Current implementation has a lot of overhead\n",
        "    - First moving tensor to CPU, then converting to numpy, then hashing\n",
        "    - Ideas of batching tensors to hash?\n",
        "  \"\"\"\n",
        "  h = hashlib.sha256()\n",
        "  h.update(x.detach().cpu().numpy().tobytes())\n",
        "  return h.hexdigest()"
      ],
      "metadata": {
        "id": "lDjR902TkyXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(hash_tensor(t_quantized))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNhBKX2-nFB9",
        "outputId": "69187ae5-26aa-4d4a-8f8b-3c54e0b3ae27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32003dadb66b1cd55f7a882232c5a30857366896188027647d7878c7f4b53ecb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ProofLinear(nn.Module):\n",
        "  \"\"\"\n",
        "  nn.Linear wrapper that logs proofs for the matrix multiplication.\n",
        "    - hashes input when forward is called, then hashes output\n",
        "    - also contained hashed weights for verification\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               in_features: int,\n",
        "               out_features: int,\n",
        "               bias: bool = True,\n",
        "               is_quantized: bool = True,\n",
        "               num_bits: int = 8):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "    if is_quantized:\n",
        "      self.weight_quantized = quantize(self.linear.weight.data, num_bits=8)\n",
        "    else:\n",
        "      self.weight_quantized = self.linear.weight.data\n",
        "\n",
        "    self.weight_hash = hash_tensor(self.weight_quantized)\n",
        "    self.proof_records = []\n",
        "    self.is_quantized = is_quantized\n",
        "    self.num_bits = num_bits\n",
        "\n",
        "  def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "    input_hash = hash_tensor(x.detach())\n",
        "\n",
        "    y = self.linear(x)\n",
        "\n",
        "    if self.is_quantized:\n",
        "      x_to_hash = quantize(x, num_bits=self.num_bits)\n",
        "      y_to_hash = quantize(y, num_bits=self.num_bits)\n",
        "    else:\n",
        "      x_to_hash, y_to_hash = x, y\n",
        "\n",
        "    input_hash = hash_tensor(x_to_hash.detach())\n",
        "    output_hash = hash_tensor(y_to_hash.detach())\n",
        "\n",
        "    dic = {\n",
        "      \"module\": \"ProofLinear\",\n",
        "      \"input\": x.detach().clone(),\n",
        "      \"output\": y.detach().clone(),\n",
        "      \"is_quantized\": self.is_quantized\n",
        "    }\n",
        "\n",
        "    if self.is_quantized:\n",
        "      dic[\"weight_quantized\"] = self.weight_quantized.clone()\n",
        "      dic[\"input_quantized\"] = x_to_hash.clone()\n",
        "      dic[\"output_quantized\"] = y_to_hash.clone()\n",
        "\n",
        "    dic[\"weight_hash\"] = self.weight_hash\n",
        "    dic[\"input_hash\"] = input_hash\n",
        "    dic[\"output_hash\"] = output_hash\n",
        "\n",
        "    self.proof_records.append(dic)\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "nPmV22g9nJIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wrap_model_linear(model: nn.Module, is_quantized: bool = True):\n",
        "  \"\"\"\n",
        "  BFS way to replace all linear layers with ProofLinear layers\n",
        "  - (haven't tested on more complex models, but should work)\n",
        "  \"\"\"\n",
        "  queue = [model]\n",
        "\n",
        "  while queue:\n",
        "    parent = queue.pop(0)\n",
        "\n",
        "    for name, child in list(parent.named_children()):\n",
        "      if isinstance(child, t.nn.Linear):\n",
        "        proof_linear = ProofLinear(\n",
        "          child.in_features,\n",
        "          child.out_features,\n",
        "          bias=(child.bias is not None),\n",
        "          is_quantized=is_quantized\n",
        "        )\n",
        "\n",
        "        proof_linear.linear.weight.data.copy_(child.weight.data)  # copying weight data\n",
        "        if child.bias is not None:\n",
        "          proof_linear.linear.bias.data.copy_(child.bias.data)\n",
        "\n",
        "        setattr(parent, name, proof_linear)\n",
        "      else:\n",
        "          queue.append(child)"
      ],
      "metadata": {
        "id": "2PfKLEgPMVfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_proof_records(model: nn.Module):\n",
        "  \"\"\"\n",
        "  Getting all proof records from the model (as each ProofLinear has the proof_records attribute)\n",
        "  \"\"\"\n",
        "  records = []\n",
        "  for m in model.modules():\n",
        "    if hasattr(m, \"proof_records\"):\n",
        "      records.extend(m.proof_records)\n",
        "\n",
        "  return records"
      ],
      "metadata": {
        "id": "qyelTVFlNnyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TensorEncoder(json.JSONEncoder):\n",
        "  \"\"\"\n",
        "  Helper class to serialize tensors to JSON\n",
        "  \"\"\"\n",
        "  def default(self, obj):\n",
        "    if isinstance(obj, t.Tensor):\n",
        "      return obj.cpu().tolist()\n",
        "\n",
        "    return super().default(obj)"
      ],
      "metadata": {
        "id": "AwTMpvFgRFZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_proof(model: nn.Module, sample_count_pct: int = 0.2, print_records: bool = False) -> bool:\n",
        "  \"\"\"\n",
        "  Gathers all proof records from the model, randomly samples some percent, recomputes hashes\n",
        "  and verifies them against the stored values\n",
        "  \"\"\"\n",
        "  proof_records = get_all_proof_records(model)\n",
        "  if not proof_records:\n",
        "    print(\"No proof records.\")\n",
        "    return False\n",
        "\n",
        "  sample_indices = random.sample(\n",
        "    range(len(proof_records)),\n",
        "    int(sample_count_pct * len(proof_records))\n",
        "  )\n",
        "  all_verified = True\n",
        "\n",
        "  for idx in sample_indices:\n",
        "    record = proof_records[idx]\n",
        "\n",
        "    # recomputing hashes\n",
        "    att = \"_quantized\" if record[\"is_quantized\"] else \"\"\n",
        "    recomputed_input_hash = hash_tensor(record[f\"input{att}\"])\n",
        "    recomputed_weight_hash = hash_tensor(record[f\"weight{att}\"])\n",
        "    recomputed_output_hash = hash_tensor(record[f\"output{att}\"])\n",
        "\n",
        "    proof_records[idx][\"recomputed_weight_hash\"] = recomputed_weight_hash\n",
        "    proof_records[idx][\"recomputed_input_hash\"] = recomputed_input_hash\n",
        "    proof_records[idx][\"recomputed_output_hash\"] = recomputed_output_hash\n",
        "\n",
        "    proof_records[idx][\"verified_weight_hash\"] = record[\"weight_hash\"] == recomputed_weight_hash\n",
        "    proof_records[idx][\"verified_input_hash\"] = record[\"input_hash\"] == recomputed_input_hash\n",
        "    proof_records[idx][\"verified_output_hash\"] = record[\"output_hash\"] == recomputed_output_hash\n",
        "\n",
        "    if not (proof_records[idx][\"verified_weight_hash\"] and\n",
        "      proof_records[idx][\"verified_input_hash\"]        and\n",
        "      proof_records[idx][\"verified_output_hash\"]):\n",
        "\n",
        "      all_verified = False\n",
        "\n",
        "  if print_records: # dumps proof records to json file\n",
        "    name = \"NA\"\n",
        "    if hasattr(model, \"name\"):\n",
        "      name = model.name\n",
        "\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "    with open(f\"proof_records_{name}_{timestamp}.json\", \"w\") as f:\n",
        "      json.dump(proof_records, f, cls=TensorEncoder, indent=4)\n",
        "\n",
        "  return all_verified"
      ],
      "metadata": {
        "id": "JZwSubTXOGld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyModel(nn.Module):\n",
        "  \"\"\"\n",
        "  Mock dummy model for testing\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(128, 64)\n",
        "    self.linear2 = nn.Linear(64, 32)\n",
        "    self.linear3 = nn.Linear(32, 16)\n",
        "    self.linear4 = nn.Linear(16, 8)\n",
        "\n",
        "  def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "    x = self.linear1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.linear2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.linear3(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.linear4(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "3ecFHjp3PdCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = DummyModel()\n",
        "test_model.name = \"test\"\n",
        "\n",
        "wrap_model_linear(test_model)\n",
        "\n",
        "test_input = t.randn(1, 128)\n",
        "test_output = test_model(test_input)\n",
        "\n",
        "verification_passed = verify_proof(test_model, sample_count_pct=1, print_records=True)\n",
        "\n",
        "print(verification_passed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I7I2UNLPkiO",
        "outputId": "4f46991b-2c2b-4894-c41a-329506f45f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    }
  ]
}