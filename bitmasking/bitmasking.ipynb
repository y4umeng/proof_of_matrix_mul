{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrkWjLhroVEu",
        "outputId": "bfd3420b-11a5-4db8-9488-08c660458d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Prover] row_idx shape: torch.Size([10]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Prover] col_idx shape: torch.Size([10]), first few: tensor([432,  32,  30,  95, 223])\n",
            "[Prover] sampled_A shape: torch.Size([10, 10000])\n",
            "[Prover] sampled_C shape: torch.Size([10, 10])\n",
            "[Prover] Sending: n_rows=10, n_cols=10\n",
            "[Verifier] Received header: layer_idx=1, n_rows=10, n_cols=10\n",
            "[Prover] Sent 400408 total bytes\n",
            "\n",
            "[Verifier] Received payload: 400400 bytes (A: 400000, C: 400)\n",
            "[Verifier] row_idx shape: torch.Size([10]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Verifier] col_idx shape: torch.Size([10]), first few: tensor([432,  32,  30,  95, 223])\n",
            "[Verifier] Max diff: 3.0517578125e-05, mean diff: 5.185604095458984e-06\n",
            "[Verifier] layer 1 passed? True\n",
            "\n",
            "[Prover] row_idx shape: torch.Size([10]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Prover] col_idx shape: torch.Size([10]), first few: tensor([432,  32,  30,  95, 223])\n",
            "[Prover] sampled_A shape: torch.Size([10, 10000])\n",
            "[Prover] sampled_C shape: torch.Size([10, 10])\n",
            "[Prover] Sending: n_rows=10, n_cols=10\n",
            "[Verifier] Received header: layer_idx=2, n_rows=10, n_cols=10\n",
            "[Prover] Sent 400408 total bytes\n",
            "\n",
            "[Verifier] Received payload: 400400 bytes (A: 400000, C: 400)\n",
            "[Verifier] row_idx shape: torch.Size([10]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Verifier] col_idx shape: torch.Size([10]), first few: tensor([432,  32,  30,  95, 223])\n",
            "[Verifier] Max diff: 3.0517578125e-05, mean diff: 5.185604095458984e-06\n",
            "[Verifier] layer 2 passed? True\n",
            "\n",
            "[Prover] row_idx shape: torch.Size([10]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Prover] col_idx shape: torch.Size([10]), first few: tensor([432,  32,  30,  95, 223])\n",
            "[Prover] sampled_A shape: torch.Size([10, 10000])\n",
            "[Prover] sampled_C shape: torch.Size([10, 10])\n",
            "[Prover] Sending: n_rows=10, n_cols=10\n",
            "[Verifier] Received header: layer_idx=3, n_rows=10, n_cols=10\n",
            "[Prover] Sent 400408 total bytes\n",
            "\n",
            "[Verifier] Received payload: 400400 bytes (A: 400000, C: 400)\n",
            "[Verifier] row_idx shape: torch.Size([10]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Verifier] col_idx shape: torch.Size([10]), first few: tensor([432,  32,  30,  95, 223])\n",
            "[Verifier] Max diff: 2.288818359375e-05, mean diff: 5.607605089608114e-06\n",
            "[Verifier] layer 3 passed? True\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import socket, struct, threading, time, random, types, contextlib\n",
        "import torch, numpy as np, torch.nn as nn\n",
        "from functools import wraps\n",
        "from torch.nn.modules.module import register_module_forward_hook\n",
        "\n",
        "\n",
        "\n",
        "HOST, PORT_BASE, GLOBAL_SEED = \"127.0.0.1\", 11234, 42\n",
        "random.seed(GLOBAL_SEED); torch.manual_seed(GLOBAL_SEED)\n",
        "\n",
        "orig_matmul = torch.matmul\n",
        "orig_tensor_matmul = torch.Tensor.__matmul__\n",
        "orig_tensor_rmatmul = torch.Tensor.__rmatmul__\n",
        "\n",
        "\n",
        "\n",
        "def _verifier_server(B_public, m, n, layer_idx, n_rows, n_cols):\n",
        "    \"\"\"Bare-bones TCP listener; exits after one blob.\"\"\"\n",
        "    def recvall(sock, n):\n",
        "        buf = bytearray()\n",
        "        while len(buf) < n:\n",
        "            chunk = sock.recv(n - len(buf))\n",
        "            if not chunk:\n",
        "                raise RuntimeError(\"socket closed\")\n",
        "            buf.extend(chunk)\n",
        "        return bytes(buf)\n",
        "\n",
        "    srv = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "    srv.bind((HOST, PORT_BASE + layer_idx))\n",
        "    srv.listen(1)\n",
        "    conn, _ = srv.accept()\n",
        "\n",
        "    hdr = recvall(conn, 8)\n",
        "    layer, rows, cols = struct.unpack(\"<HHH2x\", hdr)\n",
        "    print(f\"[Verifier] Received header: layer_idx={layer}, n_rows={rows}, n_cols={cols}\")\n",
        "\n",
        "    k = B_public.shape[0]\n",
        "    bytes_A = rows * k * 4\n",
        "    bytes_C = rows * cols * 4\n",
        "    payload = recvall(conn, bytes_A + bytes_C)\n",
        "    print(f\"[Verifier] Received payload: {len(payload)} bytes (A: {bytes_A}, C: {bytes_C})\")\n",
        "\n",
        "    A_rows = np.frombuffer(payload[:bytes_A], dtype=np.float32).reshape(rows, k)\n",
        "    C_vals = np.frombuffer(payload[bytes_A:], dtype=np.float32).reshape(rows, cols)\n",
        "\n",
        "    rng = random.Random(GLOBAL_SEED)\n",
        "    row_idx_v = torch.tensor(rng.sample(range(m), n_rows))\n",
        "    col_idx_v = torch.tensor(rng.sample(range(n), n_cols))\n",
        "\n",
        "    print(f\"[Verifier] row_idx shape: {row_idx_v.shape}, first few: {row_idx_v[:5]}\")\n",
        "    print(f\"[Verifier] col_idx shape: {col_idx_v.shape}, first few: {col_idx_v[:5]}\")\n",
        "\n",
        "    # disable hooks **inside verifier** to avoid recursion\n",
        "    _THREAD.no_hook = True\n",
        "    try:\n",
        "        B_sub      = B_public[:, col_idx_v]                    # k × cols\n",
        "        recomputed = orig_tensor_matmul(torch.from_numpy(A_rows.copy()), B_sub)\n",
        "    finally:\n",
        "        _THREAD.no_hook = False\n",
        "\n",
        "    diff = torch.abs(recomputed - torch.from_numpy(C_vals.copy()))\n",
        "    ok = torch.allclose(recomputed, torch.from_numpy(C_vals), atol=1e-3, rtol=1e-3)\n",
        "\n",
        "    print(f\"[Verifier] Max diff: {diff.max().item()}, mean diff: {diff.mean().item()}\")\n",
        "    print(f\"[Verifier] layer {layer} passed? {ok}\\n\")\n",
        "    conn.close(); srv.close()\n",
        "\n",
        "\n",
        "def _prover_send(sampled_A, sampled_C, layer_idx):\n",
        "    n_rows = sampled_A.shape[0]\n",
        "    n_cols = sampled_C.shape[1]\n",
        "    print(f\"[Prover] Sending: n_rows={n_rows}, n_cols={n_cols}\")\n",
        "\n",
        "    buf_A = sampled_A.cpu().numpy().astype(np.float32).tobytes()\n",
        "    buf_C = sampled_C.cpu().numpy().astype(np.float32).tobytes()\n",
        "    hdr = struct.pack(\"<HHH2x\", layer_idx, n_rows, n_cols)\n",
        "\n",
        "    with socket.create_connection((HOST, PORT_BASE + layer_idx)) as s:\n",
        "        s.sendall(hdr + buf_A + buf_C)\n",
        "        print(f\"[Prover] Sent {len(hdr)+len(buf_A)+len(buf_C)} total bytes\\n\")\n",
        "\n",
        "\n",
        "def audit_protocol(A, B, layer_idx):\n",
        "    if getattr(_THREAD,\"in_audit\",False):\n",
        "        return\n",
        "\n",
        "\n",
        "    _THREAD.in_audit = True\n",
        "    try:\n",
        "        # use the *un-patched* implementation exactly once\n",
        "        C = orig_matmul(A, B)\n",
        "\n",
        "        m, _ = A.shape\n",
        "        n    = B.shape[1]\n",
        "\n",
        "        rng = random.Random(GLOBAL_SEED)\n",
        "        row_idx = torch.tensor(rng.sample(range(m), max(1, int(m * 0.001))))\n",
        "        col_idx = torch.tensor(rng.sample(range(n), max(1, int(n * 0.01))))\n",
        "\n",
        "        sampled_A = A[row_idx]\n",
        "        sampled_C = C[row_idx][:, col_idx]\n",
        "\n",
        "        print(f\"[Prover] row_idx shape: {row_idx.shape}, first few: {row_idx[:5]}\")\n",
        "        print(f\"[Prover] col_idx shape: {col_idx.shape}, first few: {col_idx[:5]}\")\n",
        "        print(f\"[Prover] sampled_A shape: {sampled_A.shape}\")\n",
        "        print(f\"[Prover] sampled_C shape: {sampled_C.shape}\")\n",
        "\n",
        "        th = threading.Thread(\n",
        "            target=_verifier_server,\n",
        "            args=(B, m, n, layer_idx,\n",
        "                  sampled_A.shape[0], sampled_C.shape[1]),\n",
        "            daemon=True)\n",
        "        th.start()\n",
        "        time.sleep(0.05)\n",
        "        _prover_send(sampled_A, sampled_C, layer_idx)\n",
        "        th.join()\n",
        "    finally:\n",
        "        _THREAD.in_audit = False\n",
        "\n",
        "# -----------------  monkey-patch + public context  ------------------\n",
        "_THREAD = threading.local()\n",
        "\n",
        "def _wrap_fn(fn, op_name, cfg):\n",
        "    @wraps(fn)\n",
        "    def wrapper(*args, **kw):\n",
        "        if getattr(_THREAD, \"no_hook\", False):\n",
        "            return fn(*args, **kw)          # bypass while flag is set\n",
        "        out = fn(*args, **kw)\n",
        "        if random.random() <= cfg.sample_rate:\n",
        "            cfg.counter += 1\n",
        "            audit_protocol(args[0], args[1], cfg.counter)\n",
        "        return out\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def verification(sample_rate=0.1):\n",
        "    cfg = types.SimpleNamespace(sample_rate=sample_rate, counter=0)\n",
        "    _THREAD.records = []\n",
        "\n",
        "    patched = []\n",
        "    for name in (\"mm\", \"matmul\", \"bmm\"):\n",
        "        orig = getattr(torch, name)\n",
        "        setattr(torch, name, _wrap_fn(orig, name, cfg))\n",
        "        patched.append((torch, name, orig))\n",
        "\n",
        "     # ------------ patch tensor @-operator methods ------------------\n",
        "    def _make_tensor_patch(orig_meth):\n",
        "        @wraps(orig_meth)\n",
        "        def _tensor_mm(self, other):\n",
        "            if getattr(_THREAD, \"no_hook\", False):\n",
        "                return orig_meth(self, other)\n",
        "            out = orig_meth(self, other)\n",
        "            if random.random() <= cfg.sample_rate:\n",
        "                cfg.counter += 1\n",
        "                audit_protocol(self, other, cfg.counter)\n",
        "            return out\n",
        "        return _tensor_mm\n",
        "\n",
        "    for meth_name, orig_meth in ((\"__matmul__\", orig_tensor_matmul),\n",
        "                                 (\"__rmatmul__\", orig_tensor_rmatmul)):\n",
        "        setattr(torch.Tensor, meth_name, _make_tensor_patch(orig_meth))\n",
        "        patched.append((torch.Tensor, meth_name, orig_meth))\n",
        "\n",
        "    def _linear_hook(module, inputs, output):\n",
        "        if isinstance(module, nn.Linear) and random.random() <= cfg.sample_rate:\n",
        "            cfg.counter += 1\n",
        "            audit_protocol(inputs[0], module.weight.t(), cfg.counter)\n",
        "\n",
        "    hook_handle = register_module_forward_hook(_linear_hook)\n",
        "\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        for tgt, name, orig in patched:\n",
        "            setattr(tgt, name, orig)\n",
        "        hook_handle.remove()\n",
        "\n",
        "\n",
        "# ----------------------------  demo  --------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    A = torch.randn(10000, 10000)\n",
        "    B = torch.randn(10000, 1000)\n",
        "\n",
        "    with verification(sample_rate=1.0):\n",
        "        _ = A @ B\n",
        "        _ = torch.matmul(A,B)\n",
        "        torch.matmul(torch.randn(10000,10000), torch.randn(10000,1000))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time, torch\n",
        "\n",
        "torch.manual_seed(0)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "d = 8192\n",
        "A = torch.randn(d, d, device=device)\n",
        "B = torch.randn(d, d // 2, device=device)    # non-square keeps sizes realistic\n",
        "\n",
        "def time_one(fn):\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    fn()\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter() - t0) * 1000  # ms\n",
        "\n",
        "# warm-up\n",
        "for _ in range(5): (A @ B)\n",
        "\n",
        "baseline = time_one(lambda: (A @ B))\n",
        "\n",
        "with verification(sample_rate=1.0):          # audit *every* matmul\n",
        "    audited = time_one(lambda: (A @ B))\n",
        "\n",
        "print(f\"baseline: {baseline:.2f} ms  |  audited: {audited:.2f} ms  \"\n",
        "      f\"|  overhead: {audited - baseline:.2f} ms  ({(audited/baseline-1)*100:.1f} %)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0jtWL6woV2E",
        "outputId": "8f0e1184-6135-4c53-c99c-99a0630d9228"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-70 (_verifier_server):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-17-aabdc5875a37>\", line 57, in _verifier_server\n",
            "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Prover] row_idx shape: torch.Size([8]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Prover] col_idx shape: torch.Size([40]), first few: tensor([3456,  260,  244,  767, 1791])\n",
            "[Prover] sampled_A shape: torch.Size([8, 8192])\n",
            "[Prover] sampled_C shape: torch.Size([8, 40])\n",
            "[Prover] Sending: n_rows=8, n_cols=40\n",
            "[Verifier] Received header: layer_idx=1, n_rows=8, n_cols=40\n",
            "[Prover] Sent 263432 total bytes\n",
            "\n",
            "[Verifier] Received payload: 263424 bytes (A: 262144, C: 1280)\n",
            "[Verifier] row_idx shape: torch.Size([8]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Verifier] col_idx shape: torch.Size([40]), first few: tensor([3456,  260,  244,  767, 1791])\n",
            "baseline: 204.62 ms  |  audited: 365.48 ms  |  overhead: 160.87 ms  (78.6 %)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "d = 8192\n",
        "A = torch.randn(d, d, device=device)\n",
        "B = torch.randn(d, d // 2, device=device)\n",
        "\n",
        "def _sync():\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def time_matmuls(iters: int, *, sample_rate: float | None = None) -> float:\n",
        "    ctx = verification(sample_rate=sample_rate) if sample_rate is not None else contextlib.nullcontext()\n",
        "\n",
        "    _sync()\n",
        "    t0 = time.perf_counter()\n",
        "    with ctx:\n",
        "        for _ in range(iters):\n",
        "            _ = A @ B\n",
        "    _sync()\n",
        "    return (time.perf_counter() - t0) * 1000.0   # ms\n",
        "\n",
        "\n",
        "iters = 100\n",
        "baseline_ms = time_matmuls(iters)                    # no auditing\n",
        "audited_ms  = time_matmuls(iters, sample_rate=0.10)  # 10 % audits\n",
        "\n",
        "print(f\"{iters} matmuls — baseline: {baseline_ms:.2f} ms | \"\n",
        "      f\"audited (10 %): {audited_ms:.2f} ms | \"\n",
        "      f\"overhead: {audited_ms - baseline_ms:.2f} ms \"\n",
        "      f\"({(audited_ms / baseline_ms - 1) * 100:.1f} %)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV1iuL6nonYA",
        "outputId": "1e6dfb86-75af-4615-cf30-c3c43163af32"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-102 (_verifier_server):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-17-aabdc5875a37>\", line 57, in _verifier_server\n",
            "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Prover] row_idx shape: torch.Size([8]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Prover] col_idx shape: torch.Size([40]), first few: tensor([3456,  260,  244,  767, 1791])\n",
            "[Prover] sampled_A shape: torch.Size([8, 8192])\n",
            "[Prover] sampled_C shape: torch.Size([8, 40])\n",
            "[Prover] Sending: n_rows=8, n_cols=40\n",
            "[Verifier] Received header: layer_idx=1, n_rows=8, n_cols=40\n",
            "[Prover] Sent 263432 total bytes\n",
            "\n",
            "[Verifier] Received payload: 263424 bytes (A: 262144, C: 1280)\n",
            "[Verifier] row_idx shape: torch.Size([8]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Verifier] col_idx shape: torch.Size([40]), first few: tensor([3456,  260,  244,  767, 1791])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-103 (_verifier_server):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-17-aabdc5875a37>\", line 57, in _verifier_server\n",
            "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Prover] row_idx shape: torch.Size([8]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Prover] col_idx shape: torch.Size([40]), first few: tensor([3456,  260,  244,  767, 1791])\n",
            "[Prover] sampled_A shape: torch.Size([8, 8192])\n",
            "[Prover] sampled_C shape: torch.Size([8, 40])\n",
            "[Prover] Sending: n_rows=8, n_cols=40\n",
            "[Verifier] Received header: layer_idx=2, n_rows=8, n_cols=40\n",
            "[Prover] Sent 263432 total bytes\n",
            "\n",
            "[Verifier] Received payload: 263424 bytes (A: 262144, C: 1280)\n",
            "[Verifier] row_idx shape: torch.Size([8]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Verifier] col_idx shape: torch.Size([40]), first few: tensor([3456,  260,  244,  767, 1791])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-104 (_verifier_server):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-17-aabdc5875a37>\", line 57, in _verifier_server\n",
            "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Prover] row_idx shape: torch.Size([8]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Prover] col_idx shape: torch.Size([40]), first few: tensor([3456,  260,  244,  767, 1791])\n",
            "[Prover] sampled_A shape: torch.Size([8, 8192])\n",
            "[Prover] sampled_C shape: torch.Size([8, 40])\n",
            "[Prover] Sending: n_rows=8, n_cols=40\n",
            "[Verifier] Received header: layer_idx=3, n_rows=8, n_cols=40\n",
            "[Prover] Sent 263432 total bytes\n",
            "\n",
            "[Verifier] Received payload: 263424 bytes (A: 262144, C: 1280)\n",
            "[Verifier] row_idx shape: torch.Size([8]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Verifier] col_idx shape: torch.Size([40]), first few: tensor([3456,  260,  244,  767, 1791])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-105 (_verifier_server):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-17-aabdc5875a37>\", line 57, in _verifier_server\n",
            "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Prover] row_idx shape: torch.Size([8]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Prover] col_idx shape: torch.Size([40]), first few: tensor([3456,  260,  244,  767, 1791])\n",
            "[Prover] sampled_A shape: torch.Size([8, 8192])\n",
            "[Prover] sampled_C shape: torch.Size([8, 40])\n",
            "[Prover] Sending: n_rows=8, n_cols=40\n",
            "[Verifier] Received header: layer_idx=4, n_rows=8, n_cols=40\n",
            "[Prover] Sent 263432 total bytes\n",
            "\n",
            "[Verifier] Received payload: 263424 bytes (A: 262144, C: 1280)\n",
            "[Verifier] row_idx shape: torch.Size([8]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Verifier] col_idx shape: torch.Size([40]), first few: tensor([3456,  260,  244,  767, 1791])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-106 (_verifier_server):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-17-aabdc5875a37>\", line 57, in _verifier_server\n",
            "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Prover] row_idx shape: torch.Size([8]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Prover] col_idx shape: torch.Size([40]), first few: tensor([3456,  260,  244,  767, 1791])\n",
            "[Prover] sampled_A shape: torch.Size([8, 8192])\n",
            "[Prover] sampled_C shape: torch.Size([8, 40])\n",
            "[Prover] Sending: n_rows=8, n_cols=40\n",
            "[Verifier] Received header: layer_idx=5, n_rows=8, n_cols=40\n",
            "[Prover] Sent 263432 total bytes\n",
            "\n",
            "[Verifier] Received payload: 263424 bytes (A: 262144, C: 1280)\n",
            "[Verifier] row_idx shape: torch.Size([8]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Verifier] col_idx shape: torch.Size([40]), first few: tensor([3456,  260,  244,  767, 1791])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-107 (_verifier_server):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-17-aabdc5875a37>\", line 57, in _verifier_server\n",
            "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Prover] row_idx shape: torch.Size([8]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Prover] col_idx shape: torch.Size([40]), first few: tensor([3456,  260,  244,  767, 1791])\n",
            "[Prover] sampled_A shape: torch.Size([8, 8192])\n",
            "[Prover] sampled_C shape: torch.Size([8, 40])\n",
            "[Prover] Sending: n_rows=8, n_cols=40\n",
            "[Prover] Sent 263432 total bytes\n",
            "\n",
            "[Verifier] Received header: layer_idx=6, n_rows=8, n_cols=40\n",
            "[Verifier] Received payload: 263424 bytes (A: 262144, C: 1280)\n",
            "[Verifier] row_idx shape: torch.Size([8]), first few: tensor([1824,  409, 4506, 4012, 3657])\n",
            "[Verifier] col_idx shape: torch.Size([40]), first few: tensor([3456,  260,  244,  767, 1791])\n",
            "100 matmuls — baseline: 15592.08 ms | audited (10 %): 17767.23 ms | overhead: 2175.15 ms (14.0 %)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vwDJ2KLOnxIW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}