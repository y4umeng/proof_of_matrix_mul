{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pltwGIYryMYL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import threading\n",
        "import types\n",
        "import contextlib\n",
        "from functools import wraps\n",
        "from torch import nn\n",
        "from torch.nn import Module\n",
        "from torch.nn.modules.module import register_module_forward_hook\n",
        "import random\n",
        "import time\n",
        "import socket  # TCP sockets\n",
        "import struct # pack/unpack binary headers\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Toy example (no streaming)"
      ],
      "metadata": {
        "id": "3eTAqcIQ-U_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# seed setup\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "# WORK FROM PROVER\n",
        "\n",
        "# B is known to both parties\n",
        "# A is only known to the prover\n",
        "A = torch.randn(4000,4000) # activations\n",
        "B = torch.randn(4000,4000) # public weight matrix\n",
        "\n",
        "# full matmul\n",
        "C = A@B\n",
        "\n",
        "print(\"C.shape: \",C.shape)\n",
        "\n",
        "n_rows = C.shape[0]\n",
        "n_cols=C.shape[1]\n",
        "\n",
        "# amount of rows and columns to sample\n",
        "n_row_samples = int(n_rows*0.001)\n",
        "n_col_samples = int(n_cols * 0.01)\n",
        "\n",
        "# sampling random row and column indices\n",
        "row_indices=torch.randperm(n_rows)[:n_row_samples]\n",
        "col_indices = torch.randperm(n_cols)[:n_col_samples]\n",
        "\n",
        "# sampling the rows of matrices to be sent over\n",
        "sampled_A = A[row_indices]\n",
        "sampled_C = C[row_indices][:,col_indices] # VALUES TO BE CHECKED.\n",
        "\n",
        "\n",
        "print(\"shape of sampled_A: \",sampled_A.shape)\n",
        "print(\"sampled_C.shape: \",sampled_C.shape)\n",
        "\n",
        "# VERIFIER SIDE RECOMPUTATION\n",
        "sampled_B = B[:,col_indices]\n",
        "\n",
        "mat = sampled_A @ sampled_B\n",
        "print(\"shape of matrix recomputation: \",mat.shape)\n",
        "\n",
        "passed = torch.allclose(mat, sampled_C)\n",
        "max_diff = (mat - sampled_C).abs().max()\n",
        "print(\"max_diff: \",max_diff)\n",
        "print(passed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLrFp0o80F9u",
        "outputId": "45f9b1ce-362b-4283-e914-ec98573c1c00"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C.shape:  torch.Size([4000, 4000])\n",
            "shape of sampled_A:  torch.Size([4, 4000])\n",
            "sampled_C.shape:  torch.Size([4, 40])\n",
            "shape of matrix recomputation:  torch.Size([4, 40])\n",
            "max_diff:  tensor(4.9738e-14)\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With Streaming"
      ],
      "metadata": {
        "id": "3ajoLo9N-YKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HOST = \"127.0.0.1\"\n",
        "PORT = 1123\n",
        "\n",
        "def prepare_packet(sampled_A: torch.Tensor, sampled_C: torch.Tensor) -> bytes:\n",
        "  \"\"\"\n",
        "  1. copy gpu tensors to cpu pinned memory\n",
        "  2. extract shapes to include a tiny header\n",
        "  3. return (header_bytes, raw_bytes_A, raw_bytes_C)\n",
        "  \"\"\"\n",
        "  A_cpu = sampled_A.detach().cpu().pin_memory()\n",
        "  C_cpu = sampled_C.detach().cpu().pin_memory()\n",
        "\n",
        "  n_rows, k_dim = A_cpu.shape\n",
        "  _, n_cols = C_cpu.shape\n",
        "\n",
        "  header = struct.pack(\"<III\",n_rows,n_cols,k_dim)\n",
        "\n",
        "  raw_A = A_cpu.numpy().tobytes()\n",
        "  raw_C = C_cpu.numpy().tobytes()\n",
        "\n",
        "  return header, raw_A, raw_C\n",
        "\n",
        "# A = torch.randn(10, 16, device=\"cuda\")\n",
        "# B = torch.randn(16, 8,  device=\"cuda\")\n",
        "\n",
        "# C = A @ B\n",
        "# # sample a few rows/cols for demo\n",
        "# rows = torch.arange(3)\n",
        "# cols = torch.arange(4)\n",
        "# sampled_A = A[rows]\n",
        "# sampled_C = C[rows][:, cols]\n",
        "\n",
        "# hdr, bufA, bufC = prepare_packet(sampled_A, sampled_C)\n",
        "# print(\"Streaming random matrices\")\n",
        "# print(\"Header bytes:\", len(hdr), \"=> shapes\", struct.unpack(\"<III\", hdr))\n",
        "# print(\"bufA:\", len(bufA), \"bytes;\", \"bufC:\", len(bufC), \"bytes\")\n",
        "\n",
        "# don't know that well what this function does\n",
        "def send_packet(header: bytes, raw_A: bytes, raw_C: bytes, chunk_size: int = 1048576):\n",
        "  \"\"\"\n",
        "  open tcp connection to (host, port), then\n",
        "  1. send 12-byte header\n",
        "  2. stream raw_A in chunk_size-byte slices\n",
        "  3. stream raw_C in chunk_size-byte slices\n",
        "  \"\"\"\n",
        "\n",
        "  conn = socket.create_connection((HOST,PORT))\n",
        "  try:\n",
        "    conn.sendall(header)\n",
        "    total_A = len(raw_A)\n",
        "    offset = 0\n",
        "    while offset < total_A:\n",
        "      end = offset + chunk_size\n",
        "      conn.sendall(raw_A[offset:end])\n",
        "      offset = end\n",
        "\n",
        "    total_C = len(raw_C)\n",
        "    offset = 0\n",
        "    while offset < total_C:\n",
        "      end = offset + chunk_size\n",
        "      conn.sendall(raw_C[offset:end])\n",
        "      offset = end\n",
        "  finally:\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "# WRAPPER FUNCTION\n",
        "def new(A,B):\n",
        "  C = A @ B\n",
        "\n",
        "  A2d = A.reshape(-1,A.shape[-1])\n",
        "  C2d = C.reshape(-1,C.shape[-1])\n",
        "\n",
        "  n_rows, k_dim = A2d.shape\n",
        "  n_cols = C2d.shape[1]\n",
        "\n",
        "  # amount of rows and columns to sample\n",
        "  # entire rows of A have to be streamed over so it should be lower\n",
        "  n_row_samples = int(n_rows*0.01)\n",
        "  n_col_samples = int(n_cols * 0.01)\n",
        "\n",
        "  # sampling random row and column indices\n",
        "  row_indices=torch.randperm(n_rows)[:n_row_samples]\n",
        "  col_indices = torch.randperm(n_cols)[:n_col_samples]\n",
        "\n",
        "  # sampling the rows of matrices to be sent over\n",
        "  sampled_A = A2d[row_indices] # ENTIRE ROWS\n",
        "  sampled_C = C2d[row_indices][:,col_indices] # VALUES TO BE CHECKED.\n",
        "\n",
        "\n",
        "  # ALL STREAMING LOGIC\n",
        "  t_prep_start = time.perf_counter()\n",
        "  header, bufA, bufC = prepare_packet(sampled_A, sampled_C)\n",
        "  prep_ms = (time.perf_counter() - t_prep_start) * 1000\n",
        "  # print(f\"prepare_packet: {prep_ms:.2f} ms\")\n",
        "\n",
        "  t_send_start = time.perf_counter()\n",
        "  send_packet(header, bufA, bufC)\n",
        "  send_ms = (time.perf_counter() - t_send_start) * 1000\n",
        "  # print(f\"send_packet: {send_ms:.2f} ms\")\n",
        "  print(f\"total network overhead: {(prep_ms + send_ms):.2f} ms\")\n",
        "\n",
        "# testing out the wrapper function\n",
        "A = torch.randn(4000,4000) # activations\n",
        "B = torch.randn(4000,4000) # public weight matrix\n",
        "\n",
        "def time_matmuls(iters: int) -> float:\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    for i in range(iters):\n",
        "      # original matmul\n",
        "      _ = A @ B\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter() - t0) * 1000.0   # ms\n",
        "\n",
        "def time_sampling_matmuls(iters: int) -> float:\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    for i in range(iters):\n",
        "      # new wrapper function\n",
        "      new(A,B)\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter() - t0) * 1000.0   # ms\n",
        "\n",
        "# print(\"\\n\")\n",
        "# baseline = time_matmuls(10)\n",
        "# print(\"baseline (normal matmul) time: \", baseline, \"ms\")\n",
        "\n",
        "# print(\"\\n\")\n",
        "# new_time = time_sampling_matmuls(10)\n",
        "# print(\"NEW TIME (WITH STREAMING) time: \",new_time, \"ms\")\n",
        "\n",
        "# overhead = new_time - baseline\n",
        "# print(f\"overhead (%): {overhead/baseline:}%\")\n",
        "\n",
        "# NETWORK SETUP\n",
        "import threading, socket, struct\n",
        "\n",
        "HOST, PORT = \"127.0.0.1\", 1123\n",
        "\n",
        "def robust_server():\n",
        "    srv = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "    srv.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
        "    srv.bind((HOST, PORT))\n",
        "    srv.listen(1)\n",
        "    # print(\"[server] listening on\", HOST, PORT)\n",
        "\n",
        "    while True:\n",
        "        conn, addr = srv.accept()\n",
        "        # print(\"[server] connection from\", addr)\n",
        "        try:\n",
        "            # 1) read header (we packed rows, cols, k_dim)\n",
        "            hdr = conn.recv(12)\n",
        "            n_rows, n_cols, k_dim = struct.unpack(\"<III\", hdr)\n",
        "            # print(f\"[server] header → rows={n_rows}, cols={n_cols}, k_dim={k_dim}\")\n",
        "\n",
        "            # 2) read exactly A_bytes then C_bytes\n",
        "            # 8 if float64\n",
        "            # 4 if float32\n",
        "            A_bytes = n_rows * k_dim * 8\n",
        "            C_bytes = n_rows * n_cols * 8\n",
        "\n",
        "            buf = bytearray()\n",
        "            while len(buf) < A_bytes + C_bytes:\n",
        "                chunk = conn.recv((A_bytes + C_bytes) - len(buf))\n",
        "                if not chunk:\n",
        "                    raise RuntimeError(\"connection closed early asdf\")\n",
        "                buf.extend(chunk)\n",
        "            # print(f\"[server] received payload: {len(buf)} bytes\")\n",
        "\n",
        "            # (we're just discarding it here; a real verifier would reshape & check)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"[server] error during handling:\", e)\n",
        "        finally:\n",
        "            conn.close()\n",
        "            print(\"[server] closed connection\")\n",
        "\n",
        "# start it once, in daemon mode\n",
        "threading.Thread(target=robust_server, daemon=True).start()\n",
        "\n",
        "\n",
        "_THREAD = threading.local()\n",
        "\n",
        "# saving original matrix multiplication functions\n",
        "_orig_matmul         = torch.matmul\n",
        "_orig_tensor_matmul  = torch.Tensor.__matmul__\n",
        "_orig_tensor_rmatmul = torch.Tensor.__rmatmul__\n",
        "\n",
        "\n",
        "def _make_wrapper(orig_function):\n",
        "  \"\"\"\n",
        "  returns a wrapper function around orig_function that:\n",
        "  1. bypasses hooking if _THREAD.no_hook is True\n",
        "  2. sets flag to avoid recursive hooks.\n",
        "  3. calls orig_function to get the real results\n",
        "  4. invokes new(a,b) streaming logic\n",
        "  \"\"\"\n",
        "  @wraps(orig_function)\n",
        "  def wrapper(a,b,*args,**kwargs):\n",
        "    # if already inside a hook, just do the raw operaetion\n",
        "    if getattr(_THREAD, \"no_hook\", False):\n",
        "      return orig_function(a,b,*args,**kwargs)\n",
        "\n",
        "    # raise flag so nested matmuls aren't hooked\n",
        "    _THREAD.no_hook = True\n",
        "    try:\n",
        "      out = orig_function(a,b,*args,**kwargs)\n",
        "      new(a,b)\n",
        "\n",
        "    finally:\n",
        "      _THREAD.no_hook = False\n",
        "\n",
        "    return out\n",
        "  return wrapper\n",
        "\n",
        "def _linear_forward_hook(module: Module, inputs: tuple, output: torch.Tensor):\n",
        "  \"\"\"\n",
        "  called after every nn.linear.forward.\n",
        "  1. skip if inside another hook\n",
        "  2. pull out the inpute activations and weight matrix and call new() to stream the sampled slice\n",
        "  \"\"\"\n",
        "  if getattr(_THREAD,\"no_hook\",False):\n",
        "    return\n",
        "\n",
        "\n",
        "  if not isinstance(module, nn.Linear):\n",
        "    return\n",
        "\n",
        "  _THREAD.no_hook = True\n",
        "  try:\n",
        "    inp = inputs[0]\n",
        "    weight = module.weight.t()\n",
        "    new(inp,weight)\n",
        "  finally:\n",
        "    _THREAD.no_hook = False\n",
        "\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def streaming_audit():\n",
        "  torch.matmul = _make_wrapper(torch.matmul)\n",
        "  torch.Tensor.__matmul__ = _make_wrapper(torch.Tensor.__matmul__)\n",
        "  torch.Tensor.__rmatmul__ = _make_wrapper(torch.Tensor.__rmatmul__)\n",
        "\n",
        "  hook_handle = register_module_forward_hook(_linear_forward_hook)\n",
        "\n",
        "  try:\n",
        "    yield # returns to user code with hooks active\n",
        "  finally:\n",
        "    # unpatch everything\n",
        "    torch.matmul              = _orig_matmul\n",
        "    torch.Tensor.__matmul__   = _orig_tensor_matmul\n",
        "    torch.Tensor.__rmatmul__  = _orig_tensor_rmatmul\n",
        "\n",
        "    # 4) Remove the forward-hook\n",
        "    hook_handle.remove()\n",
        "\n",
        "# testing out the wrapper function\n",
        "A = torch.randn(4000,4000) # activations\n",
        "B = torch.randn(4000,4000) # public weight matrix\n",
        "\n",
        "def time_matmuls(iters: int) -> float:\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    for i in range(iters):\n",
        "      # original matmul\n",
        "      _ = A @ B\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter() - t0) * 1000.0   # ms\n",
        "\n",
        "def time_sampling_matmuls(iters: int) -> float:\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    with streaming_audit():\n",
        "      # Each of these operations will:\n",
        "      # 1) compute the result normally on GPU\n",
        "      # 2) call new(A, B) under the hood to stream sample\n",
        "      for i in range(iters):\n",
        "        # new wrapper function\n",
        "        new(A,B)\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter() - t0) * 1000.0   # ms\n",
        "\n",
        "# print(\"\\n\")\n",
        "# # baseline = time_matmuls(10)\n",
        "# print(\"baseline (normal matmul) time: \", baseline, \"ms\")\n",
        "\n",
        "# print(\"\\n\")\n",
        "# # new_time = time_sampling_matmuls(10)\n",
        "# print(\"NEW TIME (WITH STREAMING) time: \",new_time, \"ms\")\n",
        "\n",
        "# # overhead = new_time - baseline\n",
        "# print(f\"overhead (%): {overhead/baseline:}%\")\n",
        "\n",
        "\n",
        "class Tiny(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(10000, 5000)\n",
        "        self.l2 = nn.Linear(5000, 1000)\n",
        "    def forward(self, x):\n",
        "        return self.l2(F.relu(self.l1(x)))\n",
        "\n",
        "model = Tiny().cuda()\n",
        "x = torch.randn(1000, 10000, device=\"cuda\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def time_matmuls(iters: int) -> float:\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    for i in range(iters):\n",
        "      # original matmul\n",
        "      _ = model(x)\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter() - t0) * 1000.0   # ms\n",
        "\n",
        "@torch.no_grad()\n",
        "def time_sampling_matmuls(iters: int) -> float:\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    with streaming_audit():\n",
        "      for i in range(iters):\n",
        "        _ = model(x)\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter() - t0) * 1000.0   # ms\n",
        "\n",
        "print(\"\\n\")\n",
        "baseline = time_matmuls(3)\n",
        "print(\"baseline (normal matmul) time: \", baseline, \"ms\")\n",
        "\n",
        "print(\"\\n\")\n",
        "new_time = time_sampling_matmuls(3)\n",
        "print(\"NEW TIME (WITH STREAMING) time: \",new_time, \"ms\")\n",
        "\n",
        "overhead = new_time - baseline\n",
        "print(f\"overhead (%): {overhead/baseline:}%\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J_MuyCgi-aBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96258c75-c36a-490f-a0b5-7fae4077b2f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "baseline (normal matmul) time:  1569.0843960001075 ms\n",
            "\n",
            "\n",
            "total network overhead: 3.19 ms\n",
            "[server] closed connection\n",
            "total network overhead: 0.71 ms\n",
            "[server] closed connection\n",
            "total network overhead: 1.55 ms[server] closed connection\n",
            "\n",
            "total network overhead: 0.76 ms\n",
            "[server] closed connection\n",
            "[server] closed connectiontotal network overhead: 2.18 ms\n",
            "\n",
            "[server] closed connection\n",
            "total network overhead: 0.93 ms\n",
            "NEW TIME (WITH STREAMING) time:  2814.3754849998004 ms\n",
            "overhead (%): 0.7936418794133541%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "model.eval()\n",
        "\n",
        "BATCH   = 16\n",
        "SEQ_LEN = 32\n",
        "\n",
        "prompt_text = \"The quick brown fox jumps over the lazy dog. \" * 4\n",
        "tokens  = tokenizer(prompt_text, return_tensors=\"pt\")[\"input_ids\"][0][:SEQ_LEN]\n",
        "inputs  = tokens.unsqueeze(0).repeat(BATCH, 1).to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def time_matmuls(iters: int) -> float:\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    for i in range(iters):\n",
        "      # original matmul\n",
        "      _ = model(inputs)\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter() - t0) * 1000.0   # ms\n",
        "\n",
        "@torch.no_grad()\n",
        "def time_sampling_matmuls(iters: int) -> float:\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    with streaming_audit():\n",
        "      # Each of these operations will:\n",
        "      # 1) compute the result normally on GPU\n",
        "      # 2) call new(A, B) under the hood to stream sample\n",
        "      for i in range(iters):\n",
        "        # new wrapper function\n",
        "        _ = model(inputs)\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter() - t0) * 1000.0   # ms\n",
        "\n",
        "iters = 20\n",
        "\n",
        "print(\"\\n\")\n",
        "baseline = time_matmuls(iters)\n",
        "print(\"baseline (normal matmul) time: \", baseline, \"ms\")\n",
        "\n",
        "print(\"\\n\")\n",
        "new_time = time_sampling_matmuls(iters)\n",
        "print(\"NEW TIME (WITH STREAMING) time: \",new_time, \"ms\")\n",
        "\n",
        "overhead = new_time - baseline\n",
        "print(f\"overhead (%): {overhead/baseline:}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXpVQID4_WOM",
        "outputId": "a06de913-2286-4931-b18a-2a1a656c964c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "\n",
            "\n",
            "baseline (normal matmul) time:  12367.334844999277 ms\n",
            "\n",
            "\n",
            "[server] closed connection\n",
            "total network overhead: 0.96 ms\n",
            "[server] closed connection\n",
            "total network overhead: 0.99 ms\n",
            "[server] closed connection\n",
            "total network overhead: 1.05 ms\n",
            "[server] closed connection\n",
            "total network overhead: 0.96 ms\n",
            "[server] closed connection\n",
            "total network overhead: 0.90 ms\n",
            "[server] closed connection\n",
            "total network overhead: 0.98 ms\n",
            "[server] closed connection\n",
            "total network overhead: 0.96 ms\n",
            "[server] closed connection\n",
            "total network overhead: 0.97 ms\n",
            "[server] closed connection\n",
            "total network overhead: 0.95 ms\n",
            "[server] closed connection\n",
            "total network overhead: 0.82 ms\n",
            "[server] closed connection\n",
            "total network overhead: 0.89 ms\n",
            "[server] closed connection\n",
            "total network overhead: 0.62 ms\n",
            "[server] closed connection\n",
            "total network overhead: 0.91 ms\n",
            "total network overhead: 0.50 ms\n",
            "[server] closed connection\n",
            "total network overhead: 0.44 ms[server] closed connection\n",
            "\n",
            "total network overhead: 0.48 ms[server] closed connection\n",
            "\n",
            "total network overhead: 0.53 ms[server] closed connection\n",
            "\n",
            "total network overhead: 0.46 ms[server] closed connection\n",
            "\n",
            "total network overhead: 0.47 ms[server] closed connection\n",
            "\n",
            "total network overhead: 0.47 ms[server] closed connection\n",
            "\n",
            "NEW TIME (WITH STREAMING) time:  15566.712208999888 ms\n",
            "overhead (%): 0.25869578240571994%\n"
          ]
        }
      ]
    }
  ]
}