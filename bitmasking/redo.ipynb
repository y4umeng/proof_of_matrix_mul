{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pltwGIYryMYL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import threading\n",
        "import types\n",
        "import contextlib\n",
        "from functools import wraps\n",
        "from torch import nn\n",
        "from torch.nn import Module\n",
        "from torch.nn.modules.module import register_module_forward_hook\n",
        "import random\n",
        "import time\n",
        "import socket  # TCP sockets\n",
        "import struct # pack/unpack binary headers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Toy example (no streaming)"
      ],
      "metadata": {
        "id": "3eTAqcIQ-U_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# seed setup\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "# WORK FROM PROVER\n",
        "\n",
        "# B is known to both parties\n",
        "# A is only known to the prover\n",
        "A = torch.randn(4000,4000) # activations\n",
        "B = torch.randn(4000,4000) # public weight matrix\n",
        "\n",
        "# full matmul\n",
        "C = A@B\n",
        "\n",
        "print(\"C.shape: \",C.shape)\n",
        "\n",
        "n_rows = C.shape[0]\n",
        "n_cols=C.shape[1]\n",
        "\n",
        "# amount of rows and columns to sample\n",
        "n_row_samples = int(n_rows*0.001)\n",
        "n_col_samples = int(n_cols * 0.01)\n",
        "\n",
        "# sampling random row and column indices\n",
        "row_indices=torch.randperm(n_rows)[:n_row_samples]\n",
        "col_indices = torch.randperm(n_cols)[:n_col_samples]\n",
        "\n",
        "# sampling the rows of matrices to be sent over\n",
        "sampled_A = A[row_indices]\n",
        "sampled_C = C[row_indices][:,col_indices] # VALUES TO BE CHECKED.\n",
        "\n",
        "\n",
        "print(\"shape of sampled_A: \",sampled_A.shape)\n",
        "print(\"sampled_C.shape: \",sampled_C.shape)\n",
        "\n",
        "# VERIFIER SIDE RECOMPUTATION\n",
        "sampled_B = B[:,col_indices]\n",
        "\n",
        "mat = sampled_A @ sampled_B\n",
        "print(\"shape of matrix recomputation: \",mat.shape)\n",
        "\n",
        "passed = torch.allclose(mat, sampled_C)\n",
        "max_diff = (mat - sampled_C).abs().max()\n",
        "print(\"max_diff: \",max_diff)\n",
        "print(passed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLrFp0o80F9u",
        "outputId": "e3b42c04-a52d-4e9d-f0ab-9e2659a2f49f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C.shape:  torch.Size([4000, 4000])\n",
            "shape of sampled_A:  torch.Size([4, 4000])\n",
            "sampled_C.shape:  torch.Size([4, 40])\n",
            "shape of matrix recomputation:  torch.Size([4, 40])\n",
            "max_diff:  tensor(4.9738e-14)\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With Streaming"
      ],
      "metadata": {
        "id": "3ajoLo9N-YKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HOST = \"127.0.0.1\"\n",
        "PORT = 11234"
      ],
      "metadata": {
        "id": "J_MuyCgi-aBP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_packet(sampled_A: torch.Tensor, sampled_C: torch.Tensor) -> bytes:\n",
        "  \"\"\"\n",
        "  1. copy gpu tensors to cpu pinned memory\n",
        "  2. extract shapes to include a tiny header\n",
        "  3. return (header_bytes, raw_bytes_A, raw_bytes_C)\n",
        "  \"\"\"\n",
        "  A_cpu = sampled_A.detach().cpu().pin_memory()\n",
        "  C_cpu = sampled_C.detach().cpu().pin_memory()\n",
        "\n",
        "  n_rows, k_dim = A_cpu.shape\n",
        "  _, n_cols = C_cpu.shape\n",
        "\n",
        "  header = struct.pack(\"<III\",n_rows,n_cols,k_dim)\n",
        "\n",
        "  raw_A = A_cpu.numpy().tobytes()\n",
        "  raw_C = C_cpu.numpy().tobytes()\n",
        "\n",
        "  return header, raw_A, raw_C\n",
        "\n",
        "A = torch.randn(10, 16, device=\"cuda\")\n",
        "B = torch.randn(16, 8,  device=\"cuda\")\n",
        "\n",
        "C = A @ B\n",
        "# sample a few rows/cols for demo\n",
        "rows = torch.arange(3)\n",
        "cols = torch.arange(4)\n",
        "sampled_A = A[rows]\n",
        "sampled_C = C[rows][:, cols]\n",
        "\n",
        "hdr, bufA, bufC = prepare_packet(sampled_A, sampled_C)\n",
        "print(\"Header bytes:\", len(hdr), \"=> shapes\", struct.unpack(\"<III\", hdr))\n",
        "print(\"bufA:\", len(bufA), \"bytes;\", \"bufC:\", len(bufC), \"bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqapbGm3_Xby",
        "outputId": "d2d81163-d9a9-4c76-eb52-ef54230d3c29"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Header bytes: 12 => shapes (3, 4, 16)\n",
            "bufA: 384 bytes; bufC: 96 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def send_packet(header: bytes, raw_A: bytes, raw_C: bytes, chunk_size: int = 1048576):\n",
        "  \"\"\"\n",
        "  open tcp connection to (host, port), then\n",
        "  1. send 12-byte header\n",
        "  2. stream raw_A in chunk_size-byte slices\n",
        "  3. stream raw_C in chunk_size-byte slices\n",
        "  \"\"\"\n",
        "\n",
        "  conn = socket.create_connection((HOST,PORT))\n",
        "  try:\n",
        "    conn.sendall(header)\n",
        "    total_A = len(raw_A)\n",
        "    offset = 0\n",
        "    while offset < total_A:\n",
        "      end = offset + chunk_size\n",
        "      conn.sendall(raw_A[offset:end])\n",
        "      offset = end\n",
        "\n",
        "    total_C = len(raw_C)\n",
        "    offset = 0\n",
        "    while offset < total_C:\n",
        "      end = offset + chunk_size\n",
        "      conn.sendall(raw_C[offset:end])\n",
        "      offset = end\n",
        "  finally:\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "C-PhsvoMCIWk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def new(A,B):\n",
        "  C = A @ B\n",
        "  n_rows, n_cols = C.shape\n",
        "\n",
        "  # amount of rows and columns to sample\n",
        "  n_row_samples = int(n_rows*0.001)\n",
        "  n_col_samples = int(n_cols * 0.01)\n",
        "\n",
        "  # sampling random row and column indices\n",
        "  row_indices=torch.randperm(n_rows)[:n_row_samples]\n",
        "  col_indices = torch.randperm(n_cols)[:n_col_samples]\n",
        "\n",
        "  # sampling the rows of matrices to be sent over\n",
        "  sampled_A = A[row_indices]\n",
        "  sampled_C = C[row_indices][:,col_indices] # VALUES TO BE CHECKED.\n",
        "\n",
        "\n",
        "  # INCLUDE ALL THE STREAMING LOGIC HERE\n",
        "  t_prep_start = time.perf_counter()\n",
        "  header, bufA, bufC = prepare_packet(sampled_A, sampled_C)\n",
        "  prep_ms = (time.perf_counter() - t_prep_start) * 1000\n",
        "  print(f\"prepare_packet: {prep_ms:.2f} ms\")\n",
        "\n",
        "  t_send_start = time.perf_counter()\n",
        "  send_packet(header, bufA, bufC)\n",
        "  send_ms = (time.perf_counter() - t_send_start) * 1000\n",
        "  print(f\"send_packet: {send_ms:.2f} ms\")\n",
        "  print(f\"total network overhead: {(prep_ms + send_ms):.2f} ms\")"
      ],
      "metadata": {
        "id": "qGxVMgNX_aeX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.randn(400,4000) # activations\n",
        "B = torch.randn(4000,400) # public weight matrix\n",
        "\n",
        "def time_matmuls(iters: int) -> float:\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    for i in range(iters):\n",
        "      _ = A @ B\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter() - t0) * 1000.0   # ms\n",
        "\n",
        "def time_sampling_matmuls(iters: int) -> float:\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    for i in range(iters):\n",
        "      new(A,B)\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter() - t0) * 1000.0   # ms\n",
        "\n",
        "baseline = time_matmuls(10)\n",
        "print(baseline, \"ms\")\n",
        "\n",
        "new_time = time_sampling_matmuls(10)\n",
        "print(new_time, \"ms\")\n",
        "\n",
        "overhead = new_time - baseline\n",
        "print(f\"overhead (%): {overhead/baseline:}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4O2c5gFDVZ4",
        "outputId": "ce351267-983f-4a7a-b719-856c11bb6e64"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "241.50785700021515 ms\n",
            "prepare_packet: 0.13 ms\n",
            "send_packet: 0.28 ms\n",
            "total network overhead: 0.41 ms\n",
            "[server] connection from ('127.0.0.1', 58712)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "[server] closed connection\n",
            "prepare_packet: 0.11 ms\n",
            "send_packet: 0.20 ms\n",
            "total network overhead: 0.31 ms\n",
            "[server] connection from ('127.0.0.1', 58718)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "[server] closed connection\n",
            "prepare_packet: 0.11 ms\n",
            "[server] connection from ('127.0.0.1', 58730)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "send_packet: 0.31 ms\n",
            "total network overhead: 0.42 ms\n",
            "[server] closed connection\n",
            "prepare_packet: 0.13 ms\n",
            "[server] connection from ('127.0.0.1', 58746)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "send_packet: 0.30 ms\n",
            "total network overhead: 0.44 ms\n",
            "[server] closed connection\n",
            "prepare_packet: 0.14 ms\n",
            "send_packet: 0.19 ms\n",
            "total network overhead: 0.33 ms\n",
            "[server] connection from ('127.0.0.1', 58752)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "[server] closed connection\n",
            "prepare_packet: 0.12 ms\n",
            "send_packet: 0.17 ms\n",
            "total network overhead: 0.28 ms\n",
            "[server] connection from ('127.0.0.1', 58764)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "[server] closed connection\n",
            "prepare_packet: 0.11 ms\n",
            "send_packet: 0.19 ms\n",
            "total network overhead: 0.30 ms\n",
            "[server] connection from ('127.0.0.1', 58770)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "[server] closed connection\n",
            "prepare_packet: 0.10 ms\n",
            "send_packet: 0.16 ms\n",
            "total network overhead: 0.27 ms\n",
            "[server] connection from ('127.0.0.1', 56072)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "[server] closed connection\n",
            "prepare_packet: 0.10 ms\n",
            "[server] connection from ('127.0.0.1', 56082)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "send_packet: 0.48 ms\n",
            "total network overhead: 0.58 ms\n",
            "[server] closed connection\n",
            "prepare_packet: 0.11 ms\n",
            "send_packet: 0.18 ms\n",
            "total network overhead: 0.29 ms\n",
            "[server] connection from ('127.0.0.1', 56088)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "[server] closed connection\n",
            "238.34401199928834 ms\n",
            "overhead (%): -0.013100381247323122%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import threading, socket, struct\n",
        "\n",
        "HOST, PORT = \"127.0.0.1\", 11234\n",
        "\n",
        "def robust_server():\n",
        "    srv = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "    srv.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
        "    srv.bind((HOST, PORT))\n",
        "    srv.listen(1)\n",
        "    print(\"[server] listening on\", HOST, PORT)\n",
        "\n",
        "    while True:\n",
        "        conn, addr = srv.accept()\n",
        "        print(\"[server] connection from\", addr)\n",
        "        try:\n",
        "            # 1) read header (we packed rows, cols, k_dim)\n",
        "            hdr = conn.recv(12)\n",
        "            n_rows, n_cols, k_dim = struct.unpack(\"<III\", hdr)\n",
        "            print(f\"[server] header → rows={n_rows}, cols={n_cols}, k_dim={k_dim}\")\n",
        "\n",
        "            # 2) read exactly A_bytes then C_bytes\n",
        "            # 8 because float64\n",
        "            A_bytes = n_rows * k_dim * 8\n",
        "            C_bytes = n_rows * n_cols * 8\n",
        "\n",
        "            buf = bytearray()\n",
        "            while len(buf) < A_bytes + C_bytes:\n",
        "                chunk = conn.recv((A_bytes + C_bytes) - len(buf))\n",
        "                if not chunk:\n",
        "                    raise RuntimeError(\"connection closed early\")\n",
        "                buf.extend(chunk)\n",
        "            print(f\"[server] received payload: {len(buf)} bytes\")\n",
        "\n",
        "            # (we're just discarding it here; a real verifier would reshape & check)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"[server] error during handling:\", e)\n",
        "        finally:\n",
        "            conn.close()\n",
        "            print(\"[server] closed connection\")\n",
        "\n",
        "# start it once, in daemon mode\n",
        "threading.Thread(target=robust_server, daemon=True).start()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "funHvSXLEBWO",
        "outputId": "65a9fb2c-29d3-4611-a168-c7dc8aa6edab"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-11 (robust_server):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_THREAD = threading.local()\n",
        "\n",
        "_orig_matmul         = torch.matmul\n",
        "_orig_tensor_matmul  = torch.Tensor.__matmul__\n",
        "_orig_tensor_rmatmul = torch.Tensor.__rmatmul__\n",
        "\n",
        "def _make_warpper(orig_function):\n",
        "  \"\"\"\n",
        "  returns a wrapper function around orig_function that:\n",
        "  1. bypasses hooking if _THREAD.no_hook is True\n",
        "  2. sets flag to avoid recursive hooks.\n",
        "  3. calls orig_function to get the real results\n",
        "  4. invokes new(a,b) streaming logic\n",
        "  \"\"\"\n",
        "  @wraps(orig_function)\n",
        "  def wrapper(a,b,*args,**kwargs):\n",
        "    # if already inside a hook, just do the raw operaetion\n",
        "    if getattr(_THREAD, \"no_hook\", False):\n",
        "      return orig_function(a,b,*args,**kwargs)\n",
        "\n",
        "    # raise flag so nested matmuls aren't hooked\n",
        "    _THREAD.no_hook = True\n",
        "    try:\n",
        "      out = orig_function(a,b,*args,**kwargs)\n",
        "      new(a,b)\n",
        "\n",
        "    finally:\n",
        "      _THREAD.no_hook = False\n",
        "\n",
        "    return out\n",
        "  return wrapper"
      ],
      "metadata": {
        "id": "BevcAiWmEZJZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _linear_forward_hook(module: Module, inputs: tuple, output: torch.Tensor):\n",
        "  \"\"\"\n",
        "  called after every nn.linear.forward.\n",
        "  1. skip if inside another hook\n",
        "  2. pull out the inpute activations and weight matrix and call new() to stream the sampled slice\n",
        "  \"\"\"\n",
        "  if getattr(_THREAD,\"no_hook\",False):\n",
        "    return\n",
        "\n",
        "  _THREAD.no_hook = True\n",
        "  try:\n",
        "    inp = inputs[0]\n",
        "    weight = module.weight.t()\n",
        "    new(inp,weight)\n",
        "  finally:\n",
        "    _THREAD.no_hook = False"
      ],
      "metadata": {
        "id": "83zsWSXsJukk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@contextlib.contextmanager\n",
        "def streaming_audit():\n",
        "  torch.matmul = _make_warpper(torch.matmul)\n",
        "  torch.Tensor.__matmul__ = _make_warpper(torch.Tensor.__matmul__)\n",
        "  torch.Tensor.__rmatmul__ = _make_warpper(torch.Tensor.__rmatmul__)\n",
        "\n",
        "  hook_handle = register_module_forward_hook(_linear_forward_hook)\n",
        "\n",
        "  try:\n",
        "    yield # returns to user code with hooks active\n",
        "  finally:\n",
        "    # unpatch everything\n",
        "    torch.matmul              = _orig_matmul\n",
        "    torch.Tensor.__matmul__   = _orig_tensor_matmul\n",
        "    torch.Tensor.__rmatmul__  = _orig_tensor_rmatmul\n",
        "\n",
        "    # 4) Remove the forward-hook\n",
        "    hook_handle.remove()\n",
        ""
      ],
      "metadata": {
        "id": "9LSErr8BKpvL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.randn(4000, 4000, device=\"cuda\")\n",
        "B = torch.randn(4000, 4000, device=\"cuda\")\n",
        "\n",
        "with streaming_audit():\n",
        "    # Each of these operations will:\n",
        "    # 1) compute the result normally on GPU\n",
        "    # 2) call new(A, B) under the hood to stream samples\n",
        "    _ = A @ B\n",
        "    _ = torch.matmul(A, B)\n",
        "    # If you have an nn.Linear layer:\n",
        "    lin = torch.nn.Linear(4000, 4000).cuda()\n",
        "    _ = lin(A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Akx93TQzLz_1",
        "outputId": "0d74b5cf-9a82-451a-8b4b-eadca82ceb2f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prepare_packet: 0.23 ms\n",
            "[server] connection from ('127.0.0.1', 56108)\n",
            "[server] header → rows=4, cols=40, k_dim=4000\n",
            "[server] received payload: 64640 bytes\n",
            "send_packet: 0.59 ms\n",
            "total network overhead: 0.82 ms\n",
            "[server] closed connection\n",
            "prepare_packet: 0.26 ms\n",
            "[server] connection from ('127.0.0.1', 56118)\n",
            "[server] header → rows=4, cols=40, k_dim=4000\n",
            "send_packet: 0.61 ms\n",
            "total network overhead: 0.88 ms\n",
            "[server] received payload: 64640 bytes\n",
            "[server] closed connection\n",
            "prepare_packet: 0.27 ms\n",
            "[server] connection from ('127.0.0.1', 48842)\n",
            "[server] header → rows=4, cols=40, k_dim=4000\n",
            "[server] received payload: 64640 bytes\n",
            "send_packet: 0.64 ms\n",
            "total network overhead: 0.91 ms\n",
            "[server] closed connection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.randn(4000, 4000, device=\"cuda\")\n",
        "B = torch.randn(4000, 4000, device=\"cuda\")\n",
        "\n",
        "with streaming_audit():\n",
        "    # Each of these operations will:\n",
        "    # 1) compute the result normally on GPU\n",
        "    # 2) call new(A, B) under the hood to stream samples\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    for i in range(10):\n",
        "      torch.matmul(A,B)\n",
        "    torch.cuda.synchronize()\n",
        "    print((time.perf_counter() - t0) * 1000.0)   # ms\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "10CRJICoN0RL",
        "outputId": "f9b96688-1204-4217-8806-af17e09c8b4f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prepare_packet: 0.33 ms\n",
            "[server] connection from ('127.0.0.1', 56474)\n",
            "[server] header → rows=4, cols=40, k_dim=4000\n",
            "send_packet: 0.60 ms\n",
            "total network overhead: 0.94 ms\n",
            "[server] received payload: 64640 bytes\n",
            "[server] closed connection\n",
            "prepare_packet: 0.26 ms\n",
            "[server] connection from ('127.0.0.1', 56476)\n",
            "[server] header → rows=4, cols=40, k_dim=4000\n",
            "[server] received payload: 64640 bytes\n",
            "send_packet: 0.56 ms\n",
            "total network overhead: 0.82 ms\n",
            "[server] closed connection\n",
            "prepare_packet: 0.30 ms\n",
            "[server] connection from ('127.0.0.1', 56484)\n",
            "[server] header → rows=4, cols=40, k_dim=4000\n",
            "[server] received payload: 64640 bytes\n",
            "[server] closed connection\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ConnectionResetError",
          "evalue": "[Errno 104] Connection reset by peer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-412b6d7e7bdd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000.0\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-aae0596ccf64>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(a, b, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0m_THREAD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-ce0f61b70603>\u001b[0m in \u001b[0;36mnew\u001b[0;34m(A, B)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mt_send_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0msend_packet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0msend_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_send_start\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"send_packet: {send_ms:.2f} ms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-22a486bdbacd>\u001b[0m in \u001b[0;36msend_packet\u001b[0;34m(header, raw_A, raw_C, chunk_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_C\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_C\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 104] Connection reset by peer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.randn(400,4000) # activations\n",
        "B = torch.randn(4000,400) # public weight matrix\n",
        "\n",
        "def time_matmuls(iters: int) -> float:\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    for i in range(iters):\n",
        "      _orig_matmul(A,B)\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter() - t0) * 1000.0   # ms\n",
        "\n",
        "def time_sampling_matmuls(iters: int) -> float:\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    for i in range(iters):\n",
        "      new(A,B)\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter() - t0) * 1000.0   # ms\n",
        "\n",
        "baseline = time_matmuls(10)\n",
        "print(baseline, \"ms\")\n",
        "\n",
        "new_time = time_sampling_matmuls(10)\n",
        "print(new_time, \"ms\")\n",
        "\n",
        "overhead = new_time - baseline\n",
        "print(f\"overhead (%): {overhead/baseline:}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w289Ac5pL_qa",
        "outputId": "2d7b8ef9-15ba-490c-9f60-6161ff7b1338"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "232.21993000061047 ms\n",
            "prepare_packet: 0.12 ms\n",
            "send_packet: 0.26 ms\n",
            "total network overhead: 0.38 ms\n",
            "[server] connection from ('127.0.0.1', 49368)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "[server] closed connection\n",
            "prepare_packet: 0.15 ms\n",
            "[server] connection from ('127.0.0.1', 49384)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "send_packet: 0.37 ms\n",
            "total network overhead: 0.52 ms\n",
            "[server] closed connection\n",
            "prepare_packet: 0.18 ms\n",
            "[server] connection from ('127.0.0.1', 49392)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "[server] closed connection\n",
            "send_packet: 0.35 ms\n",
            "total network overhead: 0.53 ms\n",
            "prepare_packet: 0.11 ms\n",
            "[server] connection from ('127.0.0.1', 49404)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "send_packet: 0.31 ms\n",
            "total network overhead: 0.42 ms\n",
            "[server] closed connection\n",
            "prepare_packet: 0.11 ms\n",
            "send_packet: 0.21 ms\n",
            "total network overhead: 0.33 ms\n",
            "[server] connection from ('127.0.0.1', 49412)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "[server] closed connection\n",
            "prepare_packet: 0.12 ms\n",
            "send_packet: 0.16 ms\n",
            "total network overhead: 0.28 ms\n",
            "[server] connection from ('127.0.0.1', 49426)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "[server] closed connection\n",
            "prepare_packet: 0.10 ms\n",
            "send_packet: 0.18 ms\n",
            "total network overhead: 0.28 ms\n",
            "[server] connection from ('127.0.0.1', 49438)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "[server] closed connection\n",
            "prepare_packet: 0.10 ms\n",
            "[server] connection from ('127.0.0.1', 49450)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "send_packet: 0.44 ms\n",
            "total network overhead: 0.55 ms\n",
            "[server] closed connection\n",
            "prepare_packet: 0.12 ms\n",
            "send_packet: 0.16 ms\n",
            "total network overhead: 0.28 ms\n",
            "[server] connection from ('127.0.0.1', 49456)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "[server] closed connection\n",
            "prepare_packet: 0.11 ms\n",
            "send_packet: 0.16 ms\n",
            "total network overhead: 0.27 ms\n",
            "[server] connection from ('127.0.0.1', 49468)\n",
            "[server] header → rows=0, cols=4, k_dim=4000\n",
            "[server] received payload: 0 bytes\n",
            "[server] closed connection\n",
            "259.1591899999912 ms\n",
            "overhead (%): 0.11600752786080815%\n"
          ]
        }
      ]
    }
  ]
}